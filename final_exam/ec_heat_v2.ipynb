{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data access\n",
    "import io\n",
    "import sqlalchemy as sa\n",
    "\n",
    "# data handling\n",
    "import json\n",
    "\n",
    "# internet\n",
    "import requests\n",
    "\n",
    "# data analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import decomposition\n",
    "\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "#import scikit-learn as sk\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# system packages\n",
    "from imp import reload\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [8, 5]\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my own libs\n",
    "from libs.gov_eneff import rt\n",
    "\n",
    "rt.init('config/config.json')\n",
    "logger = rt.logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.gov_eneff import db\n",
    "reload(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.gov_eneff import preprocessing\n",
    "reload(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_columns', 30)\n",
    "pd.set_option('display.max_rows', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANG='en'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = db.DB()\n",
    "db.load(source='local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_org      = db.df_org.copy()\n",
    "df_declr    = db.df_declr.copy()\n",
    "df_bld      = db.df_bld.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bld = df_bld.merge(df_org['label'], left_on='id_org', right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of attributes to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding New Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bld['meter_heat_inputs_exists'] = df_bld['meter_heat_inputs_commercial'] > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sets of Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most complete list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_set_base = [\n",
    "    'id_voc_building_function_type',\n",
    "    'id_voc_building_type',\n",
    "    'floor_area',\n",
    "    'useful_area',\n",
    "    'heated_area',\n",
    "    'total_value',\n",
    "    'floor_amount',\n",
    "    #'launch_year',\n",
    "    #'building_age',\n",
    "    'id_voc_outer_walls_list',\n",
    "    'front_insulated',\n",
    "    'id_voc_windows_list',\n",
    "    'id_voc_windows_glazing',\n",
    "    'windows_glazing_coverage',\n",
    "    # 'windows_other', # this is a textual field\n",
    "    'id_voc_windows_wood',\n",
    "    'single_door_amount',\n",
    "    'double_door_amount',\n",
    "    'entrance_amount',\n",
    "    'vestibule_amount',\n",
    "    'door_closer_amount',\n",
    "    'air_curtain_amount',\n",
    "    'air_curtain_manual_amount',\n",
    "    'air_curtain_auto_amount',\n",
    "    'roof_exists',\n",
    "    'roof_frost',\n",
    "    'roof_insulated',\n",
    "    'roofing_metal_insulated',\n",
    "    'roofing_soft_single_insulation',\n",
    "    'attic_exists',\n",
    "    'attic_insulated',\n",
    "    'attic_pipe_insulated',\n",
    "    'technical_floor_exists',\n",
    "    'basement_exists',\n",
    "    'basement_cold',\n",
    "    'basement_damp',\n",
    "    'basement_walls_freeze',\n",
    "    'basement_glazing',\n",
    "    'basement_pipe_insulated',\n",
    "    'id_voc_boiler_control',\n",
    "    # 'meter_heat_inputs_commercial',  # replaced by meter_heat_inputs_exists\n",
    "    'id_voc_heat_piping',\n",
    "    'cast_iron_heater_amount',\n",
    "    'convectors_amount',\n",
    "    'convectors_with_thermostatic_flow_control',\n",
    "    'bimetal_heater_amount',\n",
    "    'thermostatic_control_heater_amount',\n",
    "    'individual_control_heater_amount',\n",
    "    'additional_heater_amount',\n",
    "    'other_heaters_amount',\n",
    "    'central_ventilation_exists',\n",
    "    'employees',\n",
    "    'guests',\n",
    "    'central_dispatching_exists',\n",
    "    #'heat_points',\n",
    "    # 'heat_payment',\n",
    "    # 'main_okved_code',\n",
    "    # 'main_okved_label',\n",
    "    'top_org_is_municipality',\n",
    "    # 'heat_consumption_per_heated_area',\n",
    "    # 'main_okved_code_l1',\n",
    "    'dd_okved_code_l1',\n",
    "    'ya_climate_zone',\n",
    "    'dd_climate_zone',\n",
    "    'meter_heat_inputs_exists']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_set1 = attr_set_base.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation (of Missing Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping information about what values were missing\n",
    "\n",
    "from sklearn.impute import MissingIndicator\n",
    "\n",
    "indicator = MissingIndicator()\n",
    "indicator.fit(df_bld)\n",
    "bld_missing_values = indicator.transform(df_bld)\n",
    "\n",
    "print(bld_missing_values.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_with_missing_values = round(100*df_bld[attr_set1].isna().mean().sort_values(ascending = False),1)\n",
    "attr_with_missing_values[attr_with_missing_values > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bld = preprocessing.i_bld_impute(df_bld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bld.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_with_missing_values = round(100*df_bld[attr_set1].isna().mean().sort_values(ascending = False),1)\n",
    "attr_with_missing_values[attr_with_missing_values > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bld = preprocessing.t_bld_categorize(df_bld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_bld['dd_okved_code_l1'] = df_bld['dd_okved_code_l1'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA (All Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heat Consumption but No Heat Payment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_bld['heat_consumption'].notna() & df_bld['heat_payment'].isna()).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution Over Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xx-small, x-small, small, medium, large, x-large, xx-large, larger, smaller\n",
    "\n",
    "SMALL_SIZE = 10\n",
    "MEDIUM_SIZE = 12\n",
    "LARGE_SIZE = 14\n",
    "\n",
    "plt.rc('font',   size=MEDIUM_SIZE)          # controls default text sizes\n",
    "plt.rc('axes',   titlesize=LARGE_SIZE)      # fontsize of the axes title\n",
    "plt.rc('axes',   labelsize=LARGE_SIZE)      # fontsize of the x and y labels\n",
    "plt.rc('xtick',  labelsize=LARGE_SIZE)      # fontsize of the tick labels\n",
    "plt.rc('ytick',  labelsize=LARGE_SIZE)      # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=LARGE_SIZE)       # legend fontsize\n",
    "plt.rc('figure', titlesize=LARGE_SIZE)      # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "\n",
    "dft = df_bld.groupby('year')['heat_consumption'].sum()/1000000\n",
    "\n",
    "dft.plot(kind='bar', ax=ax)\n",
    "\n",
    "if LANG =='ru':\n",
    "    ax.set_xlabel('Год')\n",
    "    ax.set_ylabel('Потребление тепла, млн. Гкал')\n",
    "elif LANG == 'en':\n",
    "    ax.set_xlabel('Year')\n",
    "    ax.set_ylabel('Heat consumption, mln. GCal')\n",
    "    \n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=0)\n",
    "#ax.ticklabel_format(style='plain')\n",
    "#plt.ticklabel_format(style='plain', axis='y')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(f'images/heating/bld_heat_consumption_years_{LANG}.png', dpi=300)\n",
    "fig.savefig(f'images/heating/bld_heat_consumption_years_{LANG}.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(df_bld.groupby('year')['heat_consumption'].sum()/1000000, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "\n",
    "dft = df_bld\\\n",
    "    .groupby('year')['heat_payment'].sum()/1000000\n",
    "\n",
    "dft.plot(kind='bar', ax=ax)\n",
    "if LANG =='ru':\n",
    "    ax.set_xlabel('Год')\n",
    "    ax.set_ylabel('Оплата тепла, млн. руб.')\n",
    "\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=0)\n",
    "#ax.ticklabel_format(style='plain')\n",
    "#plt.ticklabel_format(style='plain', axis='y')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(f'images/heating/bld_heat_payment_years_{LANG}.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bld['chk_floor_heated_area_diff'] = df_bld['floor_area'] - df_bld['heated_area']\n",
    "df_bld['chk_floor_useful_area_diff'] = df_bld['floor_area'] - df_bld['useful_area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Указанная общая площадь меньше отапливаемой\n",
    "\n",
    "df_bld.query('chk_floor_heated_area_diff < 0')\\\n",
    "    [['year', 'id_org', 'floor_area', 'heated_area', 'useful_area', 'chk_floor_heated_area_diff', 'heat_consumption']]\\\n",
    "    .sort_values('chk_floor_heated_area_diff', ascending=True)\\\n",
    "    .merge(df_org[['label']], left_on='id_org', right_index=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Указанная общая площадь меньше полезной\n",
    "\n",
    "df_bld.query('chk_floor_useful_area_diff < 0')\\\n",
    "    [['year', 'id_org', 'floor_area', 'heated_area', 'useful_area', 'chk_floor_useful_area_diff', 'heat_consumption']]\\\n",
    "    .sort_values('chk_floor_useful_area_diff', ascending=True)\\\n",
    "    .merge(df_org[['label']], left_on='id_org', right_index=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Общая площадь – нулевая, потребление тепла – есть.\n",
    "\n",
    "df_bld.query('floor_area == 0')\\\n",
    "    [['year', 'id_org', 'floor_area', 'heated_area', 'useful_area', 'heat_consumption']]\\\n",
    "    .sort_values('heat_consumption', ascending=False)\\\n",
    "    .merge(df_org[['label']], left_on='id_org', right_index=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отапиваемая площадь – нулевая, потребление тепла – есть.\n",
    "\n",
    "df_bld.query('heated_area == 0')\\\n",
    "    [['year', 'id_org', 'floor_area', 'heated_area', 'useful_area', 'heat_consumption']]\\\n",
    "    .sort_values('heat_consumption', ascending=False)\\\n",
    "    .merge(df_org[['label']], left_on='id_org', right_index=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ненормально высокое потребление (heat_consumption_per_heated_area)\n",
    "\n",
    "df_bld[df_bld['heat_consumption_per_heated_area'] > df_bld['heat_consumption_per_heated_area'].quantile(0.99)]\\\n",
    "    [['heated_area', 'heat_consumption', 'heat_consumption_per_heated_area', 'label']]\\\n",
    "    .sort_values('heat_consumption_per_heated_area', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building with High Ratio of Min/Max Consumption over Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of years\n",
    "years_count = len(df_bld['year'].drop_duplicates())\n",
    "\n",
    "# let's try to find such buildings which have too wide range over the year, \n",
    "# let's say their difference from the median is more than 50% for, at least, one year\n",
    "df_bld_ht_diff = df_bld.sort_values(['id_org_building', 'year'])\\\n",
    "    .groupby(['id_org_building'])['year', 'id_org_building', 'heat_consumption']\\\n",
    "    .filter(lambda x: len(x['heat_consumption']) == years_count and (x['heat_consumption'] > 0).all())\\\n",
    "    .groupby(['id_org_building'])['heat_consumption']\\\n",
    "    .apply(lambda x: x.max()/x.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "\n",
    "ax = sns.distplot(np.log10(df_bld_ht_diff))\n",
    "\n",
    "if LANG =='ru':\n",
    "    ax.set_title('Распределение отношения потребления тепла по годам: $r=\\log_{10}{\\\\frac{x_{max}}{x_{min}}}$')\n",
    "    ax.set_xlabel('Отношение')\n",
    "elif LANG == 'en':\n",
    "    ax.set_xlabel('r (ratio)')\n",
    "    \n",
    "ax.set_ylabel('$f(r)$')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(f'images/heating/bld_heat_consumption_ratio_distribution_{LANG}.png', dpi=300)\n",
    "fig.savefig(f'images/heating/bld_heat_consumption_ratio_distribution_{LANG}.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bld_ht_diff.loc[lambda x: x >= 100].sort_values().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bld.query('id_org_building == 38715')[['id_org', 'id_org_building', 'year', 'heat_consumption']].sort_values('year')\\\n",
    "    .merge(df_org['label'], left_on='id_org', right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bld_hc_filter1 = preprocessing.Filter()\n",
    "\n",
    "bld_hc_filter1.add(name='bld_hc_exists', fn=preprocessing.f_bld_hc_exists)\n",
    "bld_hc_filter1.add(name='bld_hc_reasonable_areas', fn=preprocessing.f_bld_hc_areas)\n",
    "bld_hc_filter1.add(name='bld_hc_consistency', fn=preprocessing.f_bld_hc_consistency)\n",
    "bld_hc_filter1.add(name='bld_hc_no_outliers', fn=preprocessing.f_bld_hc_no_outliers)\n",
    "bld_hc_filter1.add(name='bld_hc_inputs_commercial', fn=preprocessing.f_bld_hc_inputs_commercial)\n",
    "bld_hc_filter1.add(name='bld_hc_inputs_com_exists', fn=preprocessing.f_bld_hc_inputs_com_exists)\n",
    "bld_hc_filter1.add(name='bld_hc_bld_employees', fn=preprocessing.f_bld_hc_employees)\n",
    "\n",
    "print(bld_hc_filter1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bld_clean = bld_hc_filter1.transform(df_bld).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('The data left of the original after all the filters: {:.2f}%'.format(100*len(df_bld_clean)/len(df_bld), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA (Clean Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bld_clean['year'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "\n",
    "dft = df_bld_clean.groupby('year')['heat_consumption'].sum()/1000000\n",
    "\n",
    "dft.plot(kind='bar', ax=ax)\n",
    "\n",
    "if LANG =='ru':\n",
    "    ax.set_xlabel('Год')\n",
    "    ax.set_ylabel('Потребление тепла, млн. Гкал')\n",
    "elif LANG == 'en':\n",
    "    ax.set_xlabel('Year')\n",
    "    ax.set_ylabel('Heat consumption, mln. GCal')\n",
    "    \n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=0)\n",
    "#ax.ticklabel_format(style='plain')\n",
    "#plt.ticklabel_format(style='plain', axis='y')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(f'images/heating/bld_heat_consumption_years_filtered_subset_{LANG}.png', dpi=300)\n",
    "fig.savefig(f'images/heating/bld_heat_consumption_years_filtered_subset_{LANG}.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "\n",
    "dft = df_bld_clean.groupby('year')['heat_payment'].sum()/1000000\n",
    "\n",
    "dft.plot(kind='bar', ax=ax)\n",
    "\n",
    "if LANG =='ru':\n",
    "    ax.set_xlabel('Год')\n",
    "    ax.set_ylabel('Оплата тепла, млн. руб.')\n",
    "\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=0)\n",
    "#ax.ticklabel_format(style='plain')\n",
    "#plt.ticklabel_format(style='plain', axis='y')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(f'images/heating/bld_heat_payment_years_filtered_subset_{LANG}.png', dpi=300)\n",
    "fig.savefig(f'images/heating/bld_heat_payment_years_filtered_subset_{LANG}.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "\n",
    "sns.distplot(df_bld_clean['heat_consumption_per_heated_area'])\n",
    "\n",
    "if LANG =='ru':\n",
    "    ax.set_xlabel('Распределение отношения потребления тепла к площади, $ГКал/m_2$')\n",
    "elif LANG == 'en':\n",
    "    ax.set_xlabel('Heat consumption per heated area per year, $GCal/m_2$')    \n",
    "\n",
    "ax.set_ylabel('$f$')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(f'images/heating/bld_heat_consumption_per_heated_area_filtered_{LANG}.png', dpi=300)\n",
    "fig.savefig(f'images/heating/bld_heat_consumption_per_heated_area_filtered_{LANG}.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions (Boxplots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = (2017, 2018)\n",
    "dft = df_bld_clean.query('year in @years').copy()\n",
    "dft['year'] = dft['year'].cat.remove_unused_categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's build series of boxplots and regression lines per each factor re the heat consumption\n",
    "\n",
    "for col_name in (\n",
    "        'id_voc_building_function_type',\n",
    "        'id_voc_building_type',\n",
    "        'floor_area',\n",
    "        'useful_area',\n",
    "        'heated_area',\n",
    "        'total_value',\n",
    "        'floor_amount',\n",
    "        'launch_year',\n",
    "        'building_age',\n",
    "        'id_voc_outer_walls_list',\n",
    "        'front_insulated',\n",
    "        'id_voc_windows_list',\n",
    "        'id_voc_windows_glazing',\n",
    "        'windows_glazing_coverage',\n",
    "        'windows_other',\n",
    "        'id_voc_windows_wood',\n",
    "        'single_door_amount',\n",
    "        'double_door_amount',\n",
    "        'entrance_amount',\n",
    "        'vestibule_amount',\n",
    "        'door_closer_amount',\n",
    "        'air_curtain_amount',\n",
    "        'air_curtain_manual_amount',\n",
    "        'air_curtain_auto_amount',\n",
    "        'roof_exists',\n",
    "        'roof_frost',\n",
    "        'roof_insulated',\n",
    "        'roofing_metal_insulated',\n",
    "        'roofing_soft_single_insulation',\n",
    "        'attic_exists',\n",
    "        'attic_insulated',\n",
    "        'attic_pipe_insulated',\n",
    "        'technical_floor_exists',\n",
    "        'basement_exists',\n",
    "        'basement_cold',\n",
    "        'basement_damp',\n",
    "        'basement_walls_freeze',\n",
    "        'basement_glazing',\n",
    "        'basement_pipe_insulated',\n",
    "        'id_voc_boiler_control',\n",
    "        'meter_heat_inputs_commercial',\n",
    "        'id_voc_heat_piping',\n",
    "        'cast_iron_heater_amount',\n",
    "        'convectors_amount',\n",
    "        'convectors_with_thermostatic_flow_control',\n",
    "        'bimetal_heater_amount',\n",
    "        'thermostatic_control_heater_amount',\n",
    "        'individual_control_heater_amount',\n",
    "        'additional_heater_amount',\n",
    "        'other_heaters_amount',\n",
    "        'central_ventilation_exists',\n",
    "        'employees',\n",
    "        'guests',\n",
    "        'central_dispatching_exists',\n",
    "        'heat_points',\n",
    "        'heat_payment',\n",
    "        'main_okved_code',\n",
    "        'main_okved_label',\n",
    "        'top_org_is_municipality',\n",
    "        'heat_consumption_per_heated_area',\n",
    "        'main_okved_code_l1',\n",
    "        'dd_geo_lat',\n",
    "        'dd_geo_lon',\n",
    "        'dd_okved',\n",
    "        'dd_okved_code_l1',\n",
    "        'ya_climate_zone',\n",
    "        'dd_climate_zone',\n",
    "        'meter_heat_inputs_exists'\n",
    "    ):\n",
    "    \n",
    "    if pd.api.types.is_categorical_dtype(dft[col_name].dtype) or \\\n",
    "       pd.api.types.is_object_dtype(dft[col_name].dtype):\n",
    "        \n",
    "        # getting the number of distinct values for a categorical/object variable\n",
    "        num_x_values = len(dft[col_name].drop_duplicates())\n",
    "        \n",
    "    elif pd.api.types.is_bool_dtype(dft[col_name].dtype):\n",
    "        \n",
    "        num_x_values = 2\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        num_x_values = None\n",
    "        \n",
    "    if num_x_values is None:\n",
    "        fig_long_side = 6\n",
    "    else:\n",
    "        fig_long_side = max(4, int(num_x_values/1.5))\n",
    "\n",
    "    logger.debug(f'Processing {col_name}  with {num_x_values} of unique values, the figure width is {fig_width}')\n",
    "   \n",
    "    # box plot for categorical x\n",
    "    if num_x_values is not None:\n",
    "        \n",
    "        #fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(fig_long_side, 8))\n",
    "        fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(8, fig_long_side))\n",
    "        ax = axs\n",
    "        #sns.boxplot(x=col_name, y=\"heat_consumption\", hue='year', data=dft, palette=\"Set3\", ax=ax)\n",
    "        sns.boxplot(y=col_name, x=\"heat_consumption\", hue='year', data=dft, palette=\"Set3\", ax=ax)\n",
    "        \n",
    "        # checking if need to rotate the x ticks labels\n",
    "        max_tick_length = max([len(s.get_text()) for s in ax.get_xticklabels()])\n",
    "        \n",
    "        # clipping the labels if they are too lengthy\n",
    "        \n",
    "        ax.set_xticklabels([s.get_text()[:16] for s in ax.get_xticklabels()])\n",
    "        \n",
    "        if max_tick_length >= 4 and max_tick_length < 6:\n",
    "            ax.tick_params(axis='x', labelrotation=45)\n",
    "        elif max_tick_length >= 6:\n",
    "            ax.tick_params(axis='x', labelrotation=-90)\n",
    "            \n",
    "    # lmplot for numeric x\n",
    "    else:\n",
    "        \n",
    "        if len(dft.year.cat.categories) == 2:\n",
    "            markers=['o', 'x']\n",
    "        elif len(dft.year.cat.categories) == 3:\n",
    "            markers=['o', 'x', '-']            \n",
    "        else:\n",
    "            raise ValueError('Not enough markers')\n",
    "        \n",
    "        g = sns.lmplot(x=col_name, y=\"heat_consumption\", hue=\"year\", data=dft, markers=markers,\n",
    "                         scatter_kws={'alpha': 0.5, 's': 20}, x_jitter=.3, y_jitter=0.3)\n",
    "        \n",
    "        fig = g.fig\n",
    "    \n",
    "    fig.suptitle(f'{col_name}')\n",
    "    \n",
    "    #fig.canvas.draw()\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    fig.savefig(f'images/heating/bld_heating_factor_{col_name}.png', dpi=150)\n",
    "    fig.savefig(f'images/heating/bld_heating_factor_{col_name}.pdf')\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Comparisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Existence of Heat Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bld_hc_filter2 = preprocessing.Filter()\n",
    "\n",
    "bld_hc_filter2.add(name='bld_hc_exists', fn=preprocessing.f_bld_hc_exists)\n",
    "bld_hc_filter2.add(name='bld_hc_reasonable_areas', fn=preprocessing.f_bld_hc_areas)\n",
    "bld_hc_filter2.add(name='bld_hc_consistency', fn=preprocessing.f_bld_hc_consistency)\n",
    "bld_hc_filter2.add(name='bld_hc_no_outliers', fn=preprocessing.f_bld_hc_no_outliers)\n",
    "bld_hc_filter2.add(name='bld_hc_bld_employees', fn=preprocessing.f_bld_hc_employees)\n",
    "\n",
    "print(bld_hc_filter2)\n",
    "\n",
    "dft = bld_hc_filter2.transform(df_bld).copy()\n",
    "\n",
    "logger.info('The data left of the original after all the filters: {:.2f}%'.format(100*len(dft)/len(df_bld), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dft[dft['meter_heat_inputs_exists']]['heat_consumption'].dropna()\n",
    "y = dft[dft['meter_heat_inputs_exists'] == False]['heat_consumption']\n",
    "scipy.stats.mannwhitneyu(x, y, alternative='two-sided')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Front Insulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dft[dft['front_insulated'] == 0]['heat_consumption'].dropna()\n",
    "y = dft[dft['front_insulated'] == 1]['heat_consumption'].dropna()\n",
    "print(len(x))\n",
    "print(len(y))\n",
    "scipy.stats.mannwhitneyu(x, y, alternative='two-sided')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft['roof_exists'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dft[dft['roof_exists'] == 0]['heat_consumption'].dropna()\n",
    "y = dft[dft['roof_exists'] == 1]['heat_consumption'].dropna()\n",
    "print(len(x))\n",
    "print(len(y))\n",
    "scipy.stats.mannwhitneyu(x, y, alternative='less')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft['attic_exists'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dft[dft['attic_exists'] == 0]['heat_consumption'].dropna()\n",
    "y = dft[dft['attic_exists'] == 1]['heat_consumption'].dropna()\n",
    "print(len(x))\n",
    "print(len(y))\n",
    "scipy.stats.mannwhitneyu(x, y, alternative='greater')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset for Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = (2014, 2015, 2016, 2017)\n",
    "df_bld_model = df_bld_clean.query('year in @years').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bld_model['year'] = df_bld_model['year'].cat.remove_unused_categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OKVED2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_func_type(x):\n",
    "\n",
    "    okved_code = x['dd_okved']\n",
    "    \n",
    "    if okved_code.startswith('85.11'):\n",
    "        func_type = 'education_elementary'\n",
    "    elif okved_code.startswith('85.1'):\n",
    "        func_type = 'education_school'\n",
    "    elif okved_code.startswith('85.41'):\n",
    "        func_type = 'education_additional'\n",
    "    elif okved_code.startswith('86.10'):\n",
    "        func_type = 'health_hospitals'\n",
    "    elif okved_code.startswith('86'):\n",
    "        func_type = 'health_others'\n",
    "    else:\n",
    "        func_type = 'others'\n",
    "        \n",
    "    return(func_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bld_model['func_type'] = df_bld_model.apply(lambda x: get_func_type(x), axis=1).astype('category')\n",
    "df_bld_model.groupby('func_type')['func_type'].count().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_temp_dict = {\n",
    "    2014: {1: 6.0, 2: 6.9, 3: 6.2},\n",
    "    2015: {1: 6.6, 2: 7.4, 3: 6.9},\n",
    "    2016: {1: 5.8, 2: 6.7, 3: 6.0},\n",
    "    2017: {1: 5.4, 2: 6.3, 3: 5.9},\n",
    "    2018: {1: np.nan, 2: np.nan, 3: np.nan},\n",
    "}\n",
    "\n",
    "df_bld_model['avg_temp'] = df_bld_model.apply(lambda x: avg_temp_dict[x['year']][x['ya_climate_zone']], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bld_model['employees_t1']   = np.log10(df_bld_clean['employees'])\n",
    "df_bld_model['total_value_t1'] = np.log10(df_bld_clean['total_value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box-Cox Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import boxcox, inv_boxcox\n",
    "\n",
    "# normalizing the dependent variable\n",
    "\n",
    "y = df_bld_model['heat_consumption'].to_numpy()\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(12,8))\n",
    "\n",
    "g = sns.distplot(y, ax=axs[0][0])\n",
    "t = axs[0][0].set_title('KDE of $y$')\n",
    "\n",
    "r = stats.probplot(y, plot=axs[0][1], )\n",
    "\n",
    "# Box-Cox transformation\n",
    "\n",
    "# be aware of the issue with automatic findind of lambda: https://github.com/scipy/scipy/issues/6873\n",
    "# this is the reason the lambda coefficient is calculated separetely with boxcox_normmax\n",
    "bc_lmax = stats.boxcox_normmax(y, brack=(-2.1, 2.1),  method='mle')\n",
    "# sadly, boxcox_normplot does not produce the graph, mostly probable due to the issue above\n",
    "prob = stats.boxcox_normplot(y, -2, 2, plot=axs[0][2])\n",
    "axs[0][2].axvline(bc_lmax, color='r')\n",
    "axs[0][2].annotate('$\\lambda=%.2f$' % bc_lmax, (0, 0.9))\n",
    "axs[0][2].plot(prob[0], prob[1])\n",
    "\n",
    "y_bc = stats.boxcox(x=y, lmbda=bc_lmax)\n",
    "\n",
    "# manually going over a range lambda's\n",
    "# TBC\n",
    "\n",
    "g = sns.distplot(y_bc, ax=axs[1][0])\n",
    "t = axs[1][0].set_title('KDE of $boxcox(y, \\lambda)$')\n",
    "\n",
    "# print('lambda: %f, CI: [%f,%f]' % (lmax, ci[0], ci[1]))\n",
    "r = stats.probplot(y_bc, plot=axs[1][1])\n",
    "\n",
    "axs[1][2].annotate('Empty', (0.1, 0.5))\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('images/heating/regr_ht_bc_plots.png', dpi=300)\n",
    "fig.savefig('images/heating/regr_ht_bc_plots.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bld_model['heat_consumption_bc'] = y_bc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_area_attrs(df):\n",
    "    \n",
    "    return(df[['floor_area', 'heated_area']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "pca_area = decomposition.PCA(n_components=2)\n",
    "pca_area.fit(get_area_attrs(df_bld_model))\n",
    "\n",
    "print('Percentage of variance explained by each of the components: %s' % pca_area.explained_variance_ratio_)\n",
    "print('Estimated number of components: %d' % pca_area.n_components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_area(pca_area, df):\n",
    "    \n",
    "    df['area_pc1'] = pca_area.transform(get_area_attrs(df)).T[:1][0]\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_func_attrs(df):\n",
    "    \n",
    "    okved_dummies = pd.get_dummies(df['dd_okved_code_l1'], prefix='okved_code')\n",
    "    bld_type_dummies = pd.get_dummies(df['id_voc_building_type'], prefix='bld_type')\n",
    "    bld_fn_type_dummies = pd.get_dummies(df['id_voc_building_function_type'], prefix='bld_fn_type')\n",
    "    \n",
    "    dfr = pd.concat([okved_dummies, bld_type_dummies, bld_fn_type_dummies], axis=1)\n",
    "    \n",
    "    return(dfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n_components = 10\n",
    "pca_func = decomposition.PCA(n_components=n_components)\n",
    "pca_func.fit(get_func_attrs(df_bld_model))\n",
    "\n",
    "print('Percentage of variance explained by each of the components: %s' % pca_func.explained_variance_ratio_)\n",
    "print('Estimated number of components: %d' % pca_func.n_components_)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (6, 4), ncols=1)\n",
    "\n",
    "explained_variance_cumsum = np.cumsum(pca_func.explained_variance_ratio_)\n",
    "\n",
    "plt.plot(range(1,len(pca_func.explained_variance_ratio_)+1), \n",
    "         explained_variance_cumsum, color='g', lw=4, zorder=1)\n",
    "ax.set_ylim(0, 1.01)\n",
    "ax.set_xticks(np.arange(1, n_components+1))\n",
    "ax.set_yticks(np.arange(0.50, 1.00, 0.05))\n",
    "ax.set_xlabel('Number of components')\n",
    "ax.set_ylabel('Total explained variance')\n",
    "ax.hlines(0.9, 1, n_components+1, color='r')\n",
    "ax.vlines(7, 0, 1, color='r')\n",
    "\n",
    "ax.annotate('{:.2f}'.format(explained_variance_cumsum[6]), xy=(7, explained_variance_cumsum[6]), \n",
    "            xytext=(8, explained_variance_cumsum[6]-0.05),\n",
    "            arrowprops=dict(arrowstyle='->', color='black'))\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('images/heating/pca_func.png', dpi=300)\n",
    "fig.savefig('images/heating/pca_func.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_func(pca_func, df):\n",
    "    \n",
    "    df['func_pc1'], df['func_pc2'], df['func_pc3'], df['func_pc4'], \\\n",
    "    df['func_pc5'], df['func_pc6'], df['func_pc7'] = \\\n",
    "        pca_func.transform(get_func_attrs(df)).T[:7]\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cons_attrs(df):\n",
    "\n",
    "   \n",
    "    dfr = pd.concat([pd.get_dummies(df['id_voc_outer_walls_list'], prefix='outer_walls'), \n",
    "#                      pd.get_dummies(df['front_insulated'], prefix='front_insulated'),\n",
    "                     pd.get_dummies(df['id_voc_windows_list'], prefix='windows_list'), \n",
    "#                      pd.get_dummies(df['roof_exists']),\n",
    "#                      pd.get_dummies(df['roof_frost']),\n",
    "#                      pd.get_dummies(df['roof_insulated']),\n",
    "#                      pd.get_dummies(df['roofing_metal_insulated']),\n",
    "#                      pd.get_dummies(df['roofing_soft_single_insulation']),\n",
    "#                      pd.get_dummies(df['attic_exists']),\n",
    "#                      pd.get_dummies(df['attic_insulated']),\n",
    "#                      pd.get_dummies(df['attic_pipe_insulated']),\n",
    "#                      pd.get_dummies(df['technical_floor_exists']),\n",
    "#                      pd.get_dummies(df['basement_exists']),\n",
    "#                      pd.get_dummies(df['basement_cold']),\n",
    "#                      pd.get_dummies(df['basement_damp']),\n",
    "#                      pd.get_dummies(df['basement_walls_freeze']),\n",
    "#                      pd.get_dummies(df['basement_glazing']),\n",
    "#                      pd.get_dummies(df['basement_pipe_insulated'])\n",
    "                    ],    \n",
    "                    axis=1)\n",
    "\n",
    "    return(dfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n_components = 10\n",
    "pca_cons = decomposition.PCA(n_components=n_components)\n",
    "pca_cons.fit(get_cons_attrs(df_bld_model))\n",
    "\n",
    "print('Percentage of variance explained by each of the components: %s' % pca_cons.explained_variance_ratio_)\n",
    "print('Estimated number of components: %d' % pca_cons.n_components_)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 6), ncols=1)\n",
    "\n",
    "plt.plot(range(1,len(pca_cons.explained_variance_ratio_)+1), \n",
    "         np.cumsum(pca_cons.explained_variance_ratio_), color='g', lw=4, zorder=1)\n",
    "ax.set_ylim(0, 1.01)\n",
    "ax.set_yticks(np.arange(0.50, 1.00, 0.05))\n",
    "ax.set_xlabel('Number of components')\n",
    "ax.set_ylabel('Total explained variance')\n",
    "ax.hlines(0.9, 1, n_components+1, color='r')\n",
    "ax.vlines(n_components, 0, 1, color='r')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_cons(pca_cons, df):\n",
    "    \n",
    "    df['cons_pc1'], df['cons_pc2'], df['cons_pc3'] = \\\n",
    "        pca_cons.transform(get_cons_attrs(df)).T[:3]\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations Using Above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bld_clean = transform_area(pca_area, df_bld_clean)\n",
    "df_bld_clean = transform_func(pca_func, df_bld_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bld_model = transform_area(pca_area, df_bld_model)\n",
    "df_bld_model = transform_func(pca_func, df_bld_model)\n",
    "df_bld_model = transform_cons(pca_cons, df_bld_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set of Attrutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_set1 = attr_set_base.copy()\n",
    "\n",
    "attr_set1.append('employees_t1')\n",
    "attr_set1.remove('employees')\n",
    "\n",
    "attr_set1.remove('total_value')\n",
    "attr_set1.append('total_value_t1')\n",
    "\n",
    "attr_set1.append('avg_temp')\n",
    "\n",
    "attr_set1.append('area_pc1')\n",
    "attr_set1.remove('floor_area')\n",
    "attr_set1.remove('heated_area')\n",
    "attr_set1.remove('useful_area')\n",
    "\n",
    "attr_set1.remove('dd_okved_code_l1')\n",
    "attr_set1.remove('id_voc_building_type')\n",
    "attr_set1.remove('id_voc_building_function_type')\n",
    "for i in range(1, 8):\n",
    "    attr_set1.append(f'func_pc{i}')\n",
    "    \n",
    "attr_set1.append('func_type')\n",
    "\n",
    "attr_set1.remove('id_voc_outer_walls_list')\n",
    "for i in range(1, 4):\n",
    "   attr_set1.append(f'cons_pc{i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr = df_bld_model[['heat_consumption', 'heat_consumption_bc'] + attr_set1].corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "# green diverging colormap\n",
    "# cmap = sns.light_palette(\"green\", as_cmap=True)\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "g = sns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "fig = g.figure\n",
    "ax = fig.axes[0]\n",
    "\n",
    "ax.tick_params(labelsize=10)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig('images/heating/corr_heatmap.png', dpi=300)\n",
    "fig.savefig('images/heating/corr_heatmap.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the pairplot for the most strong correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = df_bld_model[['heat_consumption_bc', 'total_value_t1', 'area_pc1', 'func_pc1',\n",
    "                    'func_pc2', 'employees_t1', 'avg_temp', 'func_type']]\n",
    "\n",
    "ax = sns.pairplot(dft, kind='reg', diag_kind='kde', markers='+')\n",
    "\n",
    "fig = ax.fig\n",
    "#fig.set_size_inches(8, 8)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('images/heating/regr_ht_correlogram.png', dpi=300)\n",
    "fig.savefig('images/heating/regr_ht_correlogram.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression (Statsmodels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [2014, 2015, 2016, 2017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'heat_consumption_bc ~ '\n",
    "\n",
    "aic = 100000\n",
    "\n",
    "sm_family = sm.families.Gaussian(sm.families.links.identity)\n",
    "\n",
    "print('No  {:42s} {:6s}    {:8s} {:10}'.format('Variable', 'pvalue', 'AIC', 'Included'))\n",
    "print('----------------------------------------------------------------------------')\n",
    "\n",
    "i = 0\n",
    "\n",
    "for attr in attr_set1:\n",
    "    \n",
    "    if formula == 'heat_consumption_bc ~ ':        \n",
    "        formula_tmp = formula + attr\n",
    "    else:\n",
    "        formula_tmp = formula + ' + ' + attr\n",
    "    \n",
    "    glm_fitter = sm.formula.glm(formula_tmp,\n",
    "                                data=df_bld_model.query('year in @years'),\n",
    "                                family=sm_family,\n",
    "                                #family=sm.families.Gamma()\n",
    "                                )\n",
    "\n",
    "    glm_mod = glm_fitter.fit()\n",
    "    \n",
    "    pvalue = glm_mod.pvalues.filter(like=attr, axis='index').median()\n",
    "    aic_ratio = (aic-glm_mod.aic)/aic\n",
    "    \n",
    "    print(f'{i:2d}  {attr:42} {pvalue:1.2f}    {glm_mod.aic:6.1f}       ', end='')\n",
    "    \n",
    "    if pvalue <= 0.2 and aic_ratio >= 0.01:        \n",
    "        formula = formula_tmp\n",
    "        aic = glm_mod.aic\n",
    "        print('Yes')\n",
    "    else:\n",
    "        print('No')\n",
    "        \n",
    "    i = i + 1\n",
    "    \n",
    "print('')\n",
    "print(formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(attr_set1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formula = 'heat_consumption_bc ~ +floor_amount'\\\n",
    "#                 '+single_door_amount+employees_t1+total_value_t1+area_pc1+func_type'\\\n",
    "#                 '+cast_iron_heater_amount'\\\n",
    "#                 '+ya_climate_zone+entrance_amount*employees_t1'\\\n",
    "#                 '+front_insulated+top_org_is_municipality+cons_pc1'\n",
    "\n",
    "formula = \"heat_consumption_bc ~ entrance_amount + employees_t1 + total_value_t1 + area_pc1 + func_pc1\"\n",
    "\n",
    "glm_fitter = sm.formula.glm(formula,\n",
    "                            data=df_bld_model,\n",
    "                            family=sm_family)\n",
    "\n",
    "glm_mod = glm_fitter.fit()\n",
    "\n",
    "glm_mod.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_transform(y):\n",
    "    \n",
    "    return(inv_boxcox(y, bc_lmax))\n",
    "    #return(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### see the discussion https://stats.stackexchange.com/questions/356053/the-identity-link-function-does-not-respect-the-domain-of-the-gamma-family\n",
    "# re the warning \"The identity link function does not respect the domain of the Gamma family.\"\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(15,10))\n",
    "\n",
    "# fitted vs. observed\n",
    "y = glm_mod.model.data.orig_endog.iloc[:, 0].to_numpy()\n",
    "yhat = glm_mod.mu\n",
    "\n",
    "from statsmodels.graphics.api import abline_plot\n",
    "\n",
    "ax = axs[0][0]\n",
    "ax.scatter(yhat, y, marker='+', alpha=0.5)\n",
    "line_fit = sm.OLS(y, sm.add_constant(yhat, prepend=True)).fit()\n",
    "abline_plot(model_results=line_fit, ax=ax)\n",
    "ax.set_xlim(5, 25)\n",
    "ax.set_ylim(5, 25)\n",
    "\n",
    "ax.set_title('Fitted vs. Observed')\n",
    "if LANG == 'ru':\n",
    "    ax.set_xlabel('Моделированные')\n",
    "    ax.set_ylabel('Декларированные')\n",
    "elif LANG == 'en':\n",
    "    ax.set_xlabel('Fitted')\n",
    "    ax.set_ylabel('Observed')\n",
    "\n",
    "# CI\n",
    "#pred = mod1.get_prediction(df_bld_regr)\n",
    "#pred_summary = pred.summary_frame(alpha=0.05)\n",
    "#ax.scatter(pred_summary['mean_ci_lower'], y, color = 'red')\n",
    "\n",
    "# Residual Dependence Plot\n",
    "ax = axs[0][1]\n",
    "ax.scatter(yhat, glm_mod.resid_pearson, marker='+', alpha=0.5)\n",
    "ax.hlines(0, 0, max(np.max(yhat), 25))\n",
    "ax.set_title('Residual Dependence Plot')\n",
    "ax.set_ylabel('Pearson Residuals')\n",
    "ax.set_xlabel('Fitted values')\n",
    "ax.set_xlim(5, 25)\n",
    "ax.set_ylim(-15, 15)\n",
    "\n",
    "# Histogram of standardized deviance residuals:\n",
    "resid = glm_mod.resid_deviance.copy()\n",
    "resid_std = stats.zscore(resid)\n",
    "\n",
    "ax = axs[0][2]\n",
    "p = sns.kdeplot(resid_std, ax=ax)\n",
    "#ax.hist(resid_std, bins=25)\n",
    "ax.set_title('KDE of standardized deviance residuals')\n",
    "\n",
    "##############################################################\n",
    "# Real Values\n",
    "\n",
    "y_real = inv_transform(y)\n",
    "yhat_real = inv_transform(yhat)\n",
    "\n",
    "# Fitted vs. Observed (Real Values)\n",
    "ax = axs[1][0]\n",
    "ax.scatter(yhat_real, y_real, marker='+', alpha=0.5)\n",
    "line_fit = sm.OLS(yhat_real, sm.add_constant(y_real, prepend=True)).fit()\n",
    "abline_plot(model_results=line_fit, ax=ax)\n",
    "ax.set_xlim(0, 1200)\n",
    "ax.set_ylim(0, 1200)\n",
    "\n",
    "ax.set_title('Fitted vs. Observed')\n",
    "if LANG == 'ru':\n",
    "    ax.set_xlabel('Моделированные')\n",
    "    ax.set_ylabel('Декларированные')\n",
    "elif LANG == 'en':\n",
    "    ax.set_xlabel('Fitted')\n",
    "    ax.set_ylabel('Observed')\n",
    "    \n",
    "# Residual Dependence Plot\n",
    "\n",
    "resid_pearson_real =  inv_transform(glm_mod.resid_pearson)\n",
    "\n",
    "ax = axs[1][1]\n",
    "ax.scatter(yhat_real, resid_pearson_real, marker='+', alpha=0.5)\n",
    "ax.hlines(0, 0, max(np.max(yhat_real), 1200))\n",
    "ax.set_title('Residual Dependence Plot')\n",
    "ax.set_ylabel('Pearson Residuals')\n",
    "ax.set_xlabel('Fitted values')\n",
    "ax.set_xlim(0, 1200)\n",
    "#ax.set_ylim(-20, 50)\n",
    "\n",
    "# Histogram of standardized deviance residuals:\n",
    "resid_real = inv_transform(glm_mod.resid_deviance).dropna()\n",
    "resid_real_std = stats.zscore(resid_real)\n",
    "\n",
    "ax = axs[1][2]\n",
    "p = sns.kdeplot(resid_real_std, ax=ax)\n",
    "#ax.hist(resid_std, bins=25)\n",
    "ax.set_title('KDE of standardized deviance residuals')\n",
    "    \n",
    "fig.tight_layout()\n",
    "fig.savefig(f'images/heating/regr_ht_glm_diag_{LANG}.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(resid_pearson_real < 0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(glm_mod.resid_pearson < 0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(y), np.sum(yhat))\n",
    "print(100*np.sum((np.abs(y - yhat)/y))/len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(y_real), np.sum(yhat_real))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = glm_mod.get_prediction(df_bld_model)\n",
    "pred_summary = pred.summary_frame(alpha=0.05)\n",
    "\n",
    "df_bld_model['heat_consumption_pred']          = inv_transform(pred_summary['mean'].to_list())\n",
    "df_bld_model['heat_consumption_pred_ci_lower'] = inv_transform(pred_summary['mean_ci_lower'].to_list())\n",
    "df_bld_model['heat_consumption_pred_ci_upper'] = inv_transform(pred_summary['mean_ci_upper'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_bld_model['heat_consumption']\n",
    "yhat = df_bld_model['heat_consumption_pred']\n",
    "\n",
    "mape = 100*np.sum((np.abs(y-yhat)/y))/len(y)\n",
    "print(mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.1f}'.format\n",
    "\n",
    "df_bld_model[['id_org', 'id_org_building', 'year', 'heat_consumption', 'heat_consumption_pred', \n",
    "              'heat_consumption_pred_ci_lower', 'heat_consumption_pred_ci_upper']]\\\n",
    "    .sort_values(['id_org', 'id_org_building', 'year']).sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "\n",
    "dft = df_bld_model.groupby('year')['heat_consumption', 'heat_consumption_pred', \n",
    "                                   'heat_consumption_pred_ci_lower', 'heat_consumption_pred_ci_upper']\\\n",
    "    .agg({'heat_consumption': sum, 'heat_consumption_pred': sum, \n",
    "          'heat_consumption_pred_ci_lower': sum,\n",
    "          'heat_consumption_pred_ci_upper': sum}) / 1000000\n",
    "\n",
    "dft[['heat_consumption', 'heat_consumption_pred']].plot(kind='bar', ax=ax)\n",
    "ax.set_xlabel('Год')\n",
    "ax.set_ylabel('Потребление тепла, млн. Гкал')\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=0)\n",
    "#ax.ticklabel_format(style='plain')\n",
    "#plt.ticklabel_format(style='plain', axis='y')\n",
    "if LANG == 'ru':\n",
    "    ax.legend(['Факт', 'Прогноз'])\n",
    "elif LANG == 'en':\n",
    "    ax.legend(['Observed', 'Predicted'], loc='lower right', frameon=True)\n",
    "    \n",
    "for i, year in enumerate(dft.index.drop_duplicates().sort_values().to_numpy()):\n",
    "    ax.vlines(i+0.12, \n",
    "              dft.iloc[i]['heat_consumption_pred_ci_lower'], \n",
    "              dft.iloc[i]['heat_consumption_pred_ci_upper'], color='red', linewidth=4)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('images/heating/bld_heat_consumption_years_validated_subset_pred.png', dpi=300)\n",
    "fig.savefig('images/heating/bld_heat_consumption_years_validated_subset_pred.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = glm_mod.get_prediction(df_bld_clean)\n",
    "pred_summary = pred.summary_frame(alpha=0.05)\n",
    "\n",
    "df_bld_clean['heat_consumption_pred']          = inv_transform(pred_summary['mean'].to_list())\n",
    "df_bld_clean['heat_consumption_pred_ci_lower'] = inv_transform(pred_summary['mean_ci_lower'].to_list())\n",
    "df_bld_clean['heat_consumption_pred_ci_upper'] = inv_transform(pred_summary['mean_ci_upper'].to_list())\n",
    "\n",
    "y = df_bld_clean['heat_consumption']\n",
    "yhat = df_bld_clean['heat_consumption_pred']\n",
    "\n",
    "print(sum(y), sum(yhat))\n",
    "mape = 100*np.sum((np.abs(y-yhat)/y))/len(y)\n",
    "print(mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "\n",
    "dft = df_bld_clean.groupby('year')['heat_consumption', 'heat_consumption_pred', \n",
    "                                   'heat_consumption_pred_ci_lower', 'heat_consumption_pred_ci_upper']\\\n",
    "    .agg({'heat_consumption': sum, 'heat_consumption_pred': sum, \n",
    "          'heat_consumption_pred_ci_lower': sum,\n",
    "          'heat_consumption_pred_ci_upper': sum}) / 1000000\n",
    "\n",
    "dft[['heat_consumption', 'heat_consumption_pred']].plot(kind='bar', ax=ax)\n",
    "ax.set_xlabel('Год')\n",
    "ax.set_ylabel('Потребление тепла, млн. Гкал')\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=0)\n",
    "#ax.ticklabel_format(style='plain')\n",
    "#plt.ticklabel_format(style='plain', axis='y')\n",
    "if LANG == 'ru':\n",
    "    ax.legend(['Факт', 'Прогноз'])\n",
    "elif LANG == 'en':\n",
    "    ax.legend(['Observed', 'Predicted (with CI)'], loc='lower right', frameon=True)\n",
    "    \n",
    "for i, year in enumerate(dft.index.drop_duplicates().sort_values().to_numpy()):\n",
    "    ax.vlines(i+0.12, \n",
    "              dft.iloc[i]['heat_consumption_pred_ci_lower'], \n",
    "              dft.iloc[i]['heat_consumption_pred_ci_upper'], color='red', linewidth=3)    \n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('images/heating/bld_heat_consumption_years_pred.png', dpi=300)\n",
    "fig.savefig('images/heating/bld_heat_consumption_years_pred.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression (LASSO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_bld_model[attr_set3], \n",
    "                                                    df_bld_model['heat_consumption'].to_numpy(),\n",
    "                                                    test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "X_train_sc = min_max_scaler.fit_transform(X_train)\n",
    "y_train_sc = min_max_scaler.fit_transform(y_train.reshape(-1, 1))\n",
    "\n",
    "X_test_sc = min_max_scaler.fit_transform(X_test)\n",
    "y_test_sc = min_max_scaler.fit_transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the base estimator LassoCV since the L1 norm promotes sparsity of features.\n",
    "clf = LassoCV(cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a minimum threshold of 0.25\n",
    "sfm = SelectFromModel(clf, threshold=0)\n",
    "sfm.fit(X_train_sc, y_train_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the threshold till the number of features equals two.\n",
    "# Note that the attribute can be set directly instead of repeatedly\n",
    "\n",
    "while True:\n",
    "    \n",
    "    X_transform = sfm.transform(X_train_sc)\n",
    "    n_features = X_transform.shape[1]\n",
    "    print(f'{n_features}, {sfm.threshold}')\n",
    "    \n",
    "    if n_features <= 10:\n",
    "        print()\n",
    "        break\n",
    "    \n",
    "    sfm.threshold += 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexes\n",
    "features_selected = sfm.get_support(indices=True)\n",
    "print(features_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.iloc[:, features_selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso()\n",
    "lasso.fit(X_train.iloc[:, features_selected], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = lasso.score(X_train.iloc[:, features_selected], y_train)\n",
    "test_score  = lasso.score(X_test.iloc[:, features_selected], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_used = np.sum(lasso.coef_ != 0)\n",
    "\n",
    "print(f'Training score: {train_score}')\n",
    "print(f'Test score: {test_score}')\n",
    "print(f'Number of features used: {coeff_used}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# #############################################################################\n",
    "# Fit regression model\n",
    "n_neighbors = 5\n",
    "\n",
    "knn = neighbors.KNeighborsRegressor(n_neighbors, weights='uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = knn.fit(X_train, y_train).predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(y_, y_train, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = knn.fit(X_train, y_train).predict(X_train)\n",
    "g = sns.jointplot(y_, y_train, kind=\"reg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = knn.fit(X_train, y_train).predict(X_test)\n",
    "g = sns.jointplot(y_, y_test, kind=\"reg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html\n",
    "\n",
    "Generating CI \n",
    "- with the bootstrapping technique: https://stats.stackexchange.com/questions/183230/bootstrapping-confidence-interval-from-a-regression-prediction/218988#218988"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_set2 = [\n",
    "    'total_value',\n",
    "    'floor_amount',\n",
    "    'front_insulated',\n",
    "    'windows_glazing_coverage',\n",
    "    'single_door_amount',\n",
    "    'double_door_amount',\n",
    "    'entrance_amount',\n",
    "    'door_closer_amount',\n",
    "    'air_curtain_amount',\n",
    "    'central_ventilation_exists',\n",
    "    'central_dispatching_exists',\n",
    "    'top_org_is_municipality',\n",
    "    'ya_climate_zone',\n",
    "    'meter_heat_inputs_exists',\n",
    "    'employees',\n",
    "    'area_pc1',\n",
    "    'func_pc1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_set3 = [\n",
    "    'total_value',\n",
    "    'entrance_amount',\n",
    "    'employees',\n",
    "    'area_pc1',\n",
    "    'func_pc1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [2014, 2015, 2016, 2017]\n",
    "srv_attr_set = attr_set3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_attrs = df_bld_model.query('year in @years')[srv_attr_set].dtypes == 'category'\n",
    "cat_attrs_idx = [i for i, x in enumerate(cat_attrs.to_list()) if x]\n",
    "cat_attrs_idx_no = [i for i, x in enumerate(cat_attrs.to_list()) if not x]\n",
    "print(cat_attrs_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer([('passthrough', 'passthrough', cat_attrs_idx_no),\n",
    "                        ('one-hot', OneHotEncoder(), cat_attrs_idx)])\n",
    "\n",
    "X = ct.fit_transform(df_bld_model.query('year in @years')[srv_attr_set])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_bld_model['heat_consumption']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()  # Default behavior is to scale to [0,1]\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "X_train, X_test, y_train, y_test = X, X, y, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# svr_pipeline = make_pipeline(\n",
    "#     #MinMaxScaler(),\n",
    "#     svm.SVR(kernel='rbf', epsilon=0.01, C=800, gamma = 'scale'),\n",
    "# )\n",
    "# svr_pipeline.fit(X_train, y_train)\n",
    "# yhat = svr_pipeline.predict(X_train)\n",
    "\n",
    "# print(sum(y_train), sum(yhat))\n",
    "# mape = 100*np.sum((np.abs(y_train-yhat)/y_train))/len(y_train)\n",
    "# print(mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'kernel':['rbf'], 'C':[1, 10, 20, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]}\n",
    "\n",
    "svr = svm.SVR(gamma=\"scale\")\n",
    "clf = model_selection.GridSearchCV(svr, param_grid, cv=5, n_jobs=8)\n",
    "clf.fit(X_train, y_train.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_train_score = clf.cv_results_['mean_train_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "\n",
    "ax.plot(param_grid['C'], mean_train_score, marker='d', zorder=1)\n",
    "ax.scatter(x=param_grid['C'], y=mean_train_score, marker='d', color='red', zorder=2)\n",
    "\n",
    "ax.set_xlabel('C')\n",
    "ax.set_ylabel('$R^2$')\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.set_yticklabels(np.round(np.arange(0, 1.01, 0.2), 1))\n",
    "    \n",
    "fig.tight_layout()\n",
    "fig.savefig('images/heating/bld_heat_consumption_svr_C.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clf = clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "svr_pipeline = make_pipeline(\n",
    "    #MinMaxScaler(),\n",
    "    svm.SVR(kernel='rbf', C=100, gamma = 'scale'),\n",
    ")\n",
    "svr_pipeline.fit(X_train, y_train)\n",
    "yhat = svr_pipeline.predict(X_train)\n",
    "\n",
    "print(sum(y_train), sum(yhat))\n",
    "mape = 100*np.sum((np.abs(y_train-yhat)/y_train))/len(y_train)\n",
    "print(mape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21.7\n",
    "\n",
    "11.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bld_model['heat_consumption_pred_svr1']     = yhat\n",
    "df_bld_model['heat_consumption_pred_ci_lower'] = np.nan\n",
    "df_bld_model['heat_consumption_pred_ci_upper'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.1f}'.format\n",
    "\n",
    "df_bld_model[['id_org', 'id_org_building', 'year', 'heat_consumption', 'heat_consumption_pred_svr', \n",
    "              'heat_consumption_pred_ci_lower', 'heat_consumption_pred_ci_upper']]\\\n",
    "    .sort_values(['id_org', 'id_org_building', 'year']).sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "\n",
    "dft = df_bld_model.groupby('year')['heat_consumption', 'heat_consumption_pred_svr', \n",
    "                                   'heat_consumption_pred_ci_lower', 'heat_consumption_pred_ci_upper']\\\n",
    "    .agg({'heat_consumption': sum, 'heat_consumption_pred_svr': sum, \n",
    "          'heat_consumption_pred_ci_lower': sum,\n",
    "          'heat_consumption_pred_ci_upper': sum}) / 1000000\n",
    "\n",
    "dft[['heat_consumption', 'heat_consumption_pred_svr']].plot(kind='bar', ax=ax)\n",
    "ax.set_xlabel('Год')\n",
    "ax.set_ylabel('Потребление тепла, млн. Гкал')\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=0)\n",
    "#ax.ticklabel_format(style='plain')\n",
    "#plt.ticklabel_format(style='plain', axis='y')\n",
    "if LANG == 'ru':\n",
    "    ax.legend(['Факт', 'Прогноз'])\n",
    "elif LANG == 'en':\n",
    "    ax.legend(['Observed', 'Predicted'], loc='lower right', frameon=True)\n",
    "    \n",
    "for i, year in enumerate(dft.index.drop_duplicates().sort_values().to_numpy()):\n",
    "    ax.vlines(i+0.12, \n",
    "              dft.iloc[i]['heat_consumption_pred_ci_lower'], \n",
    "              dft.iloc[i]['heat_consumption_pred_ci_upper'], color='red', linewidth=4)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('images/heating/bld_heat_consumption_years_svr_pred.png', dpi=300)\n",
    "fig.savefig('images/heating/bld_heat_consumption_years_svr_pred.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observed vs Predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(15,5))\n",
    "\n",
    "ax = axs[0]\n",
    "y = df_bld_model['heat_consumption']\n",
    "yhat = df_bld_model['heat_consumption_pred']\n",
    "ax.scatter(yhat, y, alpha=0.5, marker='+')\n",
    "line_fit = sm.OLS(yhat_real, sm.add_constant(y, prepend=True)).fit()\n",
    "abline_plot(model_results=line_fit, ax=ax)\n",
    "ax.set_xlim(0, 1200)\n",
    "ax.set_ylim(0, 1200)\n",
    "ax.set_xlabel('Fitted')\n",
    "ax.set_ylabel('Observed')\n",
    "ax.set_title('GLM (limited set)')\n",
    "ax.text(900, 1100, 'MAPE=24.7%')\n",
    "\n",
    "ax = axs[1]\n",
    "y = df_bld_model['heat_consumption']\n",
    "yhat = df_bld_model['heat_consumption_pred_svr1']\n",
    "ax.scatter(yhat, y, alpha=0.5, marker='+')\n",
    "line_fit = sm.OLS(yhat_real, sm.add_constant(y, prepend=True)).fit()\n",
    "abline_plot(model_results=line_fit, ax=ax)\n",
    "ax.set_xlim(0, 1200)\n",
    "ax.set_ylim(0, 1200)\n",
    "ax.set_xlabel('Fitted')\n",
    "ax.set_ylabel('Observed')\n",
    "ax.set_title('SVR (limited set, C=200)')\n",
    "ax.text(900, 1100, 'MAPE=21.7%')\n",
    "\n",
    "ax = axs[2]\n",
    "y = df_bld_model['heat_consumption']\n",
    "yhat = df_bld_model['heat_consumption_pred_svr2']\n",
    "ax.scatter(yhat, y, alpha=0.5, marker='+')\n",
    "line_fit = sm.OLS(yhat_real, sm.add_constant(y, prepend=True)).fit()\n",
    "abline_plot(model_results=line_fit, ax=ax)\n",
    "ax.set_xlim(0, 1200)\n",
    "ax.set_ylim(0, 1200)\n",
    "ax.set_xlabel('Fitted')\n",
    "ax.set_ylabel('Observed')\n",
    "ax.set_title('SVR (full set, C=400)')\n",
    "ax.text(900, 1100, 'MAPE=11.7%')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('images/heating/bld_heat_consumption_all_pred.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288.516px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
