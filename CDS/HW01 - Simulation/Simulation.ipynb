{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation of Data Generation Process\n",
    "\n",
    "**The task**: \n",
    "\n",
    ">Generate four “populations” of data: normal, lightly skewed, medium-skewed, heavily-skewed. This is your dependent variable (DV). Using dependent variables (DV), generate independent variables (IV) that correlate with each DV at 0%, 10%, 30%, 50%, and 70% (so you’ll end up with 20 “populations” of IVs).\n",
    "\n",
    "The notation:\n",
    "\n",
    "* $i$ - the case number, $j$ - the number of independent variable,\n",
    "* The independent variables are $X_{ij}, i = 1..4, j = 1..5$,\n",
    "* The dependent variables are $Y_i, i = 1..4$,\n",
    "\n",
    "I use the [skewed normal distribution](https://en.wikipedia.org/wiki/Skew_normal_distribution) (with the skewness parameter $\\alpha$) to generate the samples, each sample of the size n = 10000. The skewness parameter is set as follows\n",
    "\n",
    "* $\\alpha = 0$ for the normal distribution,\n",
    "* $\\alpha = 2$ for the lighty skewed normal distribution,\n",
    "* $\\alpha = 5$ for medium-skewed normal distribution,\n",
    "* $\\alpha = 25$ for heavily-skewed normal distribution.\n",
    "\n",
    "For each of dependent random variables I generate the other five samples (at the required levels of correlation) by using the Cholesky transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "#from scipy.stats import skewnorm\n",
    "from sklearn.neighbors.kde import KernelDensity\n",
    "import statsmodels.api as sm\n",
    "\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting some global and initial parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the sample size (per case)\n",
    "total_sample_size = 1000\n",
    "\n",
    "# number of cases\n",
    "cases_no = 4\n",
    "\n",
    "# parameters of distributions\n",
    "#   skewed_0 - normal, \n",
    "#   skewed_{i} - skewed (the more skewed the greater the index is)\n",
    "norm_dist_params = {'xi': scipy.stats.skewnorm(loc = 0, scale = 1, a = 0),\n",
    "                    'y0': scipy.stats.skewnorm(loc = 0, scale = 1, a = 0),\n",
    "                    'y1': scipy.stats.skewnorm(loc = 0, scale = 1, a = 2),\n",
    "                    'y2': scipy.stats.skewnorm(loc = 0, scale = 1, a = 5),\n",
    "                    'y3': scipy.stats.skewnorm(loc = 0, scale = 1, a = 25)}\n",
    "\n",
    "# the correlation matrix: y, x1, x2, ..., x5\n",
    "corr_matrix = np.matrix([[1.0, 0.0, 0.1, 0.3, 0.5, 0.7],\n",
    "                         [0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                         [0.1, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
    "                         [0.3, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
    "                         [0.5, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
    "                         [0.7, 0.0, 0.0, 0.0, 0.0, 1.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawing samples for both the dependent and independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate samples with the Cholesky transformation\n",
    "# this procedure generate all variables at once\n",
    "\n",
    "# seed the random number generator\n",
    "np.random.seed(42)\n",
    "\n",
    "# draw random values out of those distributions\n",
    "y = []\n",
    "x = []\n",
    "x_ = []\n",
    "\n",
    "for i in range(cases_no):\n",
    "    \n",
    "    # build set of random variables\n",
    "    rv_y = norm_dist_params['y%d' % i]\n",
    "    #rv_x = norm_dist_params['xi']\n",
    "    rv_x = rv_y\n",
    "    \n",
    "    rvs = np.matrix([rv_y.rvs(size = total_sample_size),\n",
    "                     rv_x.rvs(size = total_sample_size),\n",
    "                     rv_x.rvs(size = total_sample_size),\n",
    "                     rv_x.rvs(size = total_sample_size),\n",
    "                     rv_x.rvs(size = total_sample_size),\n",
    "                     rv_x.rvs(size = total_sample_size)])\n",
    "    \n",
    "    # the transformation to align RV's at the required level of correlation\n",
    "    c = np.linalg.cholesky(corr_matrix)\n",
    "    rvs = c * rvs\n",
    "    \n",
    "    y.append(rvs[0].A[0])\n",
    "    x.append(rvs[1:].T.A)\n",
    "    x_.append(rvs[1:].A)\n",
    "    \n",
    "print('Y  shape: ' + str(np.shape(y)))\n",
    "print('X  shape: ' + str(np.shape(x)))\n",
    "print('X_ shape: ' + str(np.shape(x_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sets = []\n",
    "\n",
    "for i in range(cases_no):\n",
    "    df_sets.append(pd.DataFrame({'y': y[i], 'x1': x[i][:,0], 'x2': x[i][:,1], 'x3': x[i][:,2], 'x4': x[i][:,3], 'x5': x[i][:,4]}))\n",
    "    print(df_sets[i].head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the correlation between $Y_i$ and $X_{ij}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(cases_no):\n",
    "    \n",
    "    print('#%d      ' % i, end = '')\n",
    "    for j in range(5):\n",
    "        print('X%d      ' % j, end = '')\n",
    "    print('')    \n",
    "    \n",
    "    print('Y%d   ' % i, end = '')\n",
    "    for j in range(5):\n",
    "        print('%+0.2f   ' % (round(np.corrcoef(y[i], x[i][:, j])[0][1],2)), end = '')\n",
    "    print('')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot correlation matrix\n",
    "fig, axs = plt.subplots(figsize=(10, 10), nrows = 2, ncols = 2)\n",
    "\n",
    "axs_flatten = axs.flatten()\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "for i in range(cases_no):\n",
    "    \n",
    "    corr = df_sets[i].corr()\n",
    "    \n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.zeros_like(corr, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    \n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(corr, mask=mask, \n",
    "                annot = True, annot_kws={\"size\": 9}, fmt = '.2f',\n",
    "                cmap=cmap, vmin = -1, vmax= 1, center=0,\n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, ax = axs_flatten[i])\n",
    "    axs_flatten[i].set_title('Case %d' % i)\n",
    " \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True distribution of dependent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize = (12, 4), ncols=2)\n",
    "\n",
    "################################################################################\n",
    "# draw ideal PDF of those DV's\n",
    "################################################################################\n",
    "\n",
    "plt.sca(axs[0])\n",
    "\n",
    "x_range = np.linspace(-5, 5, total_sample_size)\n",
    "\n",
    "for i in range(cases_no):\n",
    "    # define a random variables with given parameters\n",
    "    rv = norm_dist_params['y%d' % i]\n",
    "    plt.plot(x_range, rv.pdf(x_range), lw = 2,\n",
    "             label = r'$Y_%d: \\mu$ = %1.2f, $\\sigma$ = %1.2f, $\\alpha$ = %1.2f' % \n",
    "             (i, rv.kwds['loc'], rv.kwds['scale'], rv.kwds['a']))\n",
    "\n",
    "plt.title('PDF for normal skewed distribution (defined by alpha)')\n",
    "plt.legend(fontsize = 8)\n",
    "\n",
    "################################################################################\n",
    "# draw kernel distribuion estimation of the samples\n",
    "################################################################################\n",
    "\n",
    "plt.sca(axs[1])\n",
    "\n",
    "for i in range(cases_no):\n",
    "    kde = sm.nonparametric.KDEUnivariate(y[i])\n",
    "    # kde.fit(kernel = 'epa', fft = False)\n",
    "    kde.fit() # gaussian kernel with fft\n",
    "    plt.plot(kde.support, kde.density, lw = 2, label='$Y_%d$' % i)\n",
    "\n",
    "plt.title('Kernel density estimation for the samples')\n",
    "plt.legend(fontsize = 8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, there are four sets of samples, one set per each required combination of DV and IVs in the variables y and x. Let's do some simulation with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation (Linear Regression of Samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The task:**\n",
    "\n",
    ">Your simulations should do the following:\n",
    ">1.\tCreate a linear regression model with one DV and 5 IVs (such that x1 correlates 10% with DV, x2 – 30%, etc.)\n",
    ">2.\tGenerate 1000 (or more, if you wish) samples each of size 15, 30, 50, 100 for normal and skewed DVs\n",
    ">3.\tFor each sample, run a linear regression and record R2, F-value, and p-values of each IV \n",
    ">4.\tAnalyze the results. Does non-normality have an impact on Type I and Type II error rates?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We understand the task is that we need to run the simulation for 1-4 for each set of DV and IVs (we have four of them, let's call them cases). Variability among runs of the experiment is defined by randomness of samples and their size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating linear regression models, the models are stored in the *models* list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "models = []\n",
    "fits   = []\n",
    "\n",
    "for i in range(cases_no):\n",
    "    models.append(sm.OLS(y[i], sm.add_constant(x[i])))\n",
    "    fits.append(models[i].fit())\n",
    "    \n",
    "print(fits[0].summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating major structures to work with those small but numerous samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of iterations\n",
    "iter_no = 1000\n",
    "\n",
    "# sizes of samples\n",
    "sample_sizes = (15, 30, 50, 100)\n",
    "\n",
    "# samples in the following structure \n",
    "# {'case id': \n",
    "#     {'15':  <list of small samples>,\n",
    "#      '30':  <list of small samples>,\n",
    "#      '50':  <list of small samples>,\n",
    "#      '100': <list of small samples>}}\n",
    "\n",
    "samples = {}\n",
    "\n",
    "# sample OLS fits\n",
    "samples_fits = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just checking the logic of randomly selecting those small samples is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control\n",
    "\n",
    "print(y[0][:5])\n",
    "print(x[0][:5,:])\n",
    "\n",
    "indexes = [1, 2]\n",
    "print(np.take(y[0], indexes))\n",
    "print(np.take(x[0], indexes, axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "for case in range(cases_no):\n",
    "    samples[case] = {}\n",
    "    for sample_size in sample_sizes:\n",
    "        samples[case][sample_size] = []\n",
    "        for j in range(iter_no):\n",
    "            # randomly generating indexes for those observations we'll take for the current sample\n",
    "            indexes = np.random.randint(0, total_sample_size - 1, sample_size)\n",
    "            ys = np.take(y[case], indexes)\n",
    "            xs = np.take(x[case], indexes, axis = 0)\n",
    "            samples[case][sample_size].append([ys, xs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking what a small sample looks like. It should be a list of two arrays: Y - the first one (1D), X - the second one (2D)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Y')\n",
    "print()\n",
    "print(samples[0][15][0][0])\n",
    "print()\n",
    "print('X1 - X5')\n",
    "print()\n",
    "print(samples[0][15][0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the models to those small samples. The results are gettings into the *fits* variable. It has the same structure as *samples*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for case in range(cases_no):\n",
    "    samples_fits[case] = {}\n",
    "    for sample_size in sample_sizes:\n",
    "        samples_fits[case][sample_size] = []\n",
    "        for sample in samples[case][sample_size]:\n",
    "            ys = sample[0]\n",
    "            xs = sample[1]\n",
    "            m = sm.OLS(ys, sm.add_constant(xs)).fit()\n",
    "            samples_fits[case][sample_size].append(m)\n",
    "            \n",
    "#print(m1.rsquared)\n",
    "#print(m1.fvalue)\n",
    "#print(m1.pvalues)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the distribution of each statistic for the linear regression models per each case and sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}\n",
    "\n",
    "# stats in the following structure \n",
    "# {<case id>: \n",
    "#     {<sample size>: \n",
    "#         {'rsquared': list,\n",
    "#          'fvalue':   list,\n",
    "#          'pvalues':  list}}}\n",
    "\n",
    "for case in range(cases_no):\n",
    "    stats[case] = {}\n",
    "    for sample_size in sample_sizes:\n",
    "        stats[case][sample_size] = {}\n",
    "        stats[case][sample_size]['rsquared'] = [sample.rsquared for sample in samples_fits[case][sample_size]]\n",
    "        stats[case][sample_size]['fvalue']   = [sample.fvalue   for sample in samples_fits[case][sample_size]]\n",
    "        stats[case][sample_size]['pvalues']  = [sample.pvalues  for sample in samples_fits[case][sample_size]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_fits[0][15][0].pvalues[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize = (10, 10), nrows = 2, ncols=2)\n",
    "\n",
    "axs_flatten = axs.flatten()\n",
    "\n",
    "################################################################################\n",
    "# draw ideal PDF of those DV's\n",
    "################################################################################\n",
    "\n",
    "for i in range(len(sample_sizes)):\n",
    "    sample_size = sample_sizes[i]\n",
    "    plt.sca(axs_flatten[i])\n",
    "    for case in range(cases_no):\n",
    "        kde = sm.nonparametric.KDEUnivariate(stats[case][sample_size]['rsquared'])\n",
    "        # kde.fit(kernel = 'epa', fft = False)\n",
    "        kde.fit() # gaussian kernel with fft\n",
    "        plt.plot(kde.support, kde.density, lw = 2, label='$Y_%d$' % case)\n",
    "        plt.legend(fontsize = 8)\n",
    "        plt.title('sample size: %d' % sample_size, size = 10)\n",
    "\n",
    "plt.suptitle('$R^2$')        \n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize = (10, 10), nrows = 2, ncols=2)\n",
    "\n",
    "axs_flatten = axs.flatten()\n",
    "\n",
    "################################################################################\n",
    "# draw ideal PDF of those DV's\n",
    "################################################################################\n",
    "\n",
    "for i in range(len(sample_sizes)):\n",
    "    sample_size = sample_sizes[i]\n",
    "    plt.sca(axs_flatten[i])\n",
    "    for case in range(cases_no):\n",
    "        kde = sm.nonparametric.KDEUnivariate(stats[case][sample_size]['fvalue'])\n",
    "        # kde.fit(kernel = 'epa', fft = False)\n",
    "        kde.fit() # gaussian kernel with fft\n",
    "        plt.plot(kde.support, kde.density, lw = 2, label='$Y_%d$' % case)\n",
    "        plt.legend(fontsize = 8)\n",
    "        plt.title('sample size: %d' % sample_size, size = 10)\n",
    "\n",
    "plt.suptitle('$F$')        \n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize = (12, 12), nrows = 4, ncols=5)\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "\n",
    "axs_flatten = axs.flatten()\n",
    "\n",
    "################################################################################\n",
    "# draw ideal PDF of those DV's\n",
    "################################################################################\n",
    "\n",
    "for i in range(len(sample_sizes)):\n",
    "    sample_size = sample_sizes[i]    \n",
    "    for j in range(5):\n",
    "        plt.sca(axs[i][j])\n",
    "        for case in range(cases_no):\n",
    "            # getting p-values for the case, this sample size, this variable\n",
    "            p = [pvalues[j+1] for pvalues in stats[case][sample_size]['pvalues']]\n",
    "            \n",
    "            #kde = sm.nonparametric.KDEUnivariate(p)\n",
    "            #kde.fit() # gaussian kernel with fft\n",
    "            #plt.plot(kde.support, kde.density, lw = 2, label='$Y_%d$' % case)\n",
    "            \n",
    "            sns.distplot(p, hist = False, label='$Y_%d$' % case)\n",
    "            \n",
    "            plt.legend(fontsize = 8)\n",
    "            plt.title('sample size: %d, IV: %d' % (sample_size, j), size = 8)\n",
    "\n",
    "plt.suptitle('$p-values$')\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "On the Cholesky transformation\n",
    "\n",
    "1. http://code.activestate.com/recipes/576512-generating-correlated-random-numbers/\n",
    "2. https://www.quantumforest.com/2011/10/simulating-data-following-a-given-covariance-structure/\n",
    "3. https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Drawing_values_from_the_distribution\n",
    "4. https://blogs.sas.com/content/iml/2012/02/08/use-the-cholesky-transformation-to-correlate-and-uncorrelate-variables.html\n",
    "5. https://math.stackexchange.com/questions/163470/generating-correlated-random-numbers-why-does-cholesky-decomposition-work\n",
    "\n",
    "Other\n",
    "\n",
    "5. http://www.statsmodels.org/dev/examples/notebooks/generated/kernel_density.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix A The Cholesky Transformation\n",
    "\n",
    "Ref.: http://code.activestate.com/recipes/576512-generating-correlated-random-numbers/\n",
    "\n",
    "For two corelated variables, the formula is much as one would get from intuition about the meaning of correlation with some twist due to normalizing the standard deviation: $X_3 = \\alpha X_1 + \\sqrt{1-\\alpha^2} X_2$ Where $X_1$ and $X_2$ are two independent random variables, and $\\alpha$ is the coefficient of correlation between $X_1$ and $X_3$.\n",
    "\n",
    "In a more general sense: \n",
    "Let $C$ be the correlation matrix desired. Let $X_1, X_2..., X_N$ be $N$ independent random variables arranged in a row matrix $R = [X_1, X_2,....,X_N]$. Then $Q = RU$ where $U^TU = C$ gives us $N$ random variables $Q = [Y_1, Y_2, ..., Y_N]$ with the required property."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
