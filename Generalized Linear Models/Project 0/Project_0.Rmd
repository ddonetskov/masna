---
title: 'GLM: Project 0'
author: 'Dina Yakovleva, Dmitry Donetskov, Elizaveta Chernenko, Ekaterina Korolkova'
#date: "`r format(Sys.Date())`"
#date: "2019-01-05"
output:
  html_document:
    # figure options
    dev: png
    fig.align: center
    fig_width: 7
    fig_height: 4
    #fig_retina: 2
    # TOC options
    toc: TRUE
    number_sections: true
    toc_depth: 2
    toc_float:
      collapsed: false
    # others
    keep_md: FALSE
    theme: spacelab   # https://bootswatch.com/3/
  pdf_document:
    #geometry: "papersize={210mm,5000mm},left=3cm,right=3cm,top=2cm,bottom=2cm"
    # figure options
    fig_caption: yes
    fig_height: 7
    fig_width: 7
    fig_retina: 2
    # TOC options
    number_sections: yes
    toc: yes
    toc_depth: 2
    # others
editor_options: 
  chunk_output_type: inline
---

<style>
  .main-container {
    max-width: 1200px !important;
  }
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.align = "center")
knitr::opts_chunk$set(comment = NA)
knitr::opts_chunk$set(dpi = 300)

library(MASS)
library(tidyverse)
library(readxl)

library(ggplot2)
library(GGally)
library(ggfortify)

# chunk option dev="png" is the default raster graphics format for HTML output
#knitr::opts_chunk$set(dev="png")

# chunk option dev="svg" produces very large vector graphics files
# knitr::opts_chunk$set(dev="pdf")

# more optimized svg, ligher in size, https://cran.r-project.org/web/packages/svglite/index.html
library(svglite)
knitr::opts_chunk$set(
  dev = "svglite",
  fig.ext = ".svg"
)
```

# Research Question

Are there region-specific differences in the Forced Vital Capacity (FVC) and the Forced Expiratory Volume in 1 second (FEV1)? 

The dataset consists of the variables

- *age* (years),
- *height* (cm),
- *weight* (kg),
- *FVC* (centiliter), Forced Vital Capacity: the volume of air that can forcibly be blown out, after full inspiration,
- *FEV1* (centiliter), Forced Expiratory Volume in 1 second: the volume of air that can forcibly be blown out in first 1 second, after full inspiration,
- *region*: M - Murau (plenty of woods, no industries), A - Aichfeld (lots of industries).

# Loading Data

The data is loaded into a data frame and the *region* variable is factorized.

```{r data_load, message=FALSE}
data  <- read_excel("data/Spirometry.xlsx", sheet = "Tabelle1")

# factorization of the region field
data$region <- factor(data$region, levels = c('A', 'M'), labels = c('A', 'M'))
```

Listing a random sample of the data just to get an idea of what it looks like.

```{r}
data[sample(1:nrow(data), 10, replace = F), ]
```

There are two more variables in the dataset than explained in the project statement: *year* and *FEV1.VC*. Noting for the time being they might be of an interest for further investigations.

# Explanatory Data Analysis

Getting the summary of the data set.

```{r}
# Summary of the non-caterogical inputs
summary(data[, c('region', 'age', 'height', 'weight', 'FVC', 'FEV1')])
```

There is nothing unusual, there are ~30% less cases for Aichfeld than for Murau (34 vs. 45) but given the number of cases per each region (tens) the data looks acceptably balanced. It's also interesting to note the age range does not cover the whole possible range e.g. children and seniors are not included in the dataset. This limits the generalization of our answer to the research question as it will only be applicable to a subset of the population.

Building the pairwise comparision matrix to understand the distribution of variables
and their interaction with each other. All variables in the plots are 'groupped' by *region*.

```{r eda_corr_plot, fig.width=8, fig.height=8, each=FALSE, message=FALSE}
p <- ggpairs(data, columns = c('region', 'age', 'height', 'weight', 'FVC', 'FEV1'),
        mapping = aes(color = region), legend = 1,
        upper = list(continuous = wrap("cor", size = 3)),
        diag  = list(continuous = wrap("densityDiag", alpha=0.4)),
        lower = list(continuous = wrap("smooth", alpha = 0.5, size=0.3))) +
  theme(legend.position = "bottom")

p
```

From the plots, we can derive some findings

- the respondents are naturally of different *age*, *height* and *weight* and the distributions of those differ by *region* threfore we shouldn't directly compare estimaties of *FVC*, *FEV1* between the regions (e.g. the means with the t-test) without taking that difference into account.
- *age* is not distributed normally, there are more young people, considerably more for Aichfeld, we need to look into whether there is any anomaly with it,
- *height*, *weight* are close to the normal distribution,
- *height*, *weight* are not of a strong linear correlation, they are probably both required for the model,
- there is a strong linear correlation between *FVC* and *FEV1*, so it might be enough to build a model for only either of them to answer the research question,
- *FVC* is the right-skewed, *FEV1* is of a slight bi-modal distribution; therefore, *FVC* may be a better response variable then as it can be easier linearized but the distribution of *FEV1* might mean there is a statistically significant difference between the regions (hence, the two peaks in the distribution).

The interim conclusion is we are going to regress for both *FVC* and *FEV1* to find out which one would be more suitlable for a linear regression model.

Let's look closer at the relations of *age*, *height* and *weight* with *FEV1*, *FVC* because we are going to consider the linear regression based on the formers. Plotting the covariates against *FEV1* and *FVC*. The green line is the linear regression one.

```{r, fig.width=8, fig.height=6, each=FALSE, message=FALSE}
gs <-list(NA,NA,NA,NA,NA,NA)

i <- 1
for (rv in c('FVC', 'FEV1')) {
  for (pv in c('age', 'height', 'weight')) {
    g <- ggplot(data = data, aes_string(x = pv, y = rv), alpha = 0.7) +
      geom_point(aes(color = region), size = 0.5) +
      geom_smooth(method = 'lm', formula = y ~ x, color = 'light green', se = FALSE) +
      geom_smooth()
    gs[[i]] <- g
    i <- i + 1
  }
}

pm <- ggmatrix(
  gs, nrow = 2, ncol = 3,
  xAxisLabels = c('age', 'height', 'weight'),
  yAxisLabels = c('FVC', 'FEV1'),
  title = "Pairwise Comparison of Predictors and Responses"
  )

pm
```

The relationship between *age* and either of *FVC*, *FEV1* is not linear due to inclusion of considerable number of observations of young people for the Aichfeld region. Also, the LOESS regression suggests that both *FVC*, *FEV1* increase until some age around 22-25 and start decreasing after that. At this point, it would be interesting to check it with a local physician if that's generally true for the population. If that is not, the dataset is not probably very representative.

Let's check the *age* distribution by the regions, if there is any suspicious in that.

```{r, fig.width=7, fig.height=3}
ggplot(data, aes(x = age, y = region, color = region), alpha = 0.7) +
  geom_jitter(size = 0.7) +
  theme_bw()
```

As we have already spotted, there is unusual high number of cases in the lower range of *age* for Aichfeld. Let's find out the exact values, calculating the number of people at different ages by the regions.

```{r}
data[, c('age', 'region')] %>% 
  count(age, region) %>% 
  spread(key = region, value = n, fill = '-') %>% 
  arrange(age) %>% 
  print(n = 100)
```

There are outliers and/or missing data in terms of *age*: certain *age* ranges are presented only for one of regions:

- there is a considerable number of observations (n = 11) with *age*=16 for Aichfeld whereas there are no cases for this region within the 21-25 age range,
- there are no cases over the age of 49 for Murau which might complement the case with the age 56 for Aichfeld.

That disbalance in the data might be due to a sampling error. The case with *age*=16 being the worst threat to getting an unbiased answer definetely requires some attention. We have considered either remove the subset or to transform it by replacing it with just one case consisting of the median values, and decided to go with the latter option.

Checking what the statistics are for *age*=16.

```{r}
data %>% filter(age == 16) %>% select(-ID, -year, -FEV1.VC) %>% summary
```

Building the new dataset which consists of all cases with *age* not equal 16 and one artifical case replacing the 11 cases with *age* equal 16 (it gets ID=100). The new dataset is stored in the **data2** variable.

```{r}
#data2 <- data %>% filter(age > 16) %>% select(-year, -FEV1.VC)

data2 <- bind_rows(
  data %>% 
    select(-year, -FEV1.VC) %>% 
    filter(age == 16) %>% 
    summarize(ID=100, age=median(age), 
              height=median(height), weight=median(weight), 
              FVC=median(FVC), FEV1=median(FEV1), 
              region=(data$region)[1]),
  data %>% 
    select(-year, -FEV1.VC) %>%
    filter(age > 16))
```

Listing the artificial case just to check ourselves it contains sensible values.

```{r}
data2 %>% filter(ID == 100)
```

Checking the relationship between the variables in the new dataset.

```{r, fig.width=8, fig.height=6}
gs <-list(NA,NA,NA,NA,NA,NA)

i <- 1
for (rv in c('FVC', 'FEV1')) {
  for (pv in c('age', 'height', 'weight')) {
    g <- ggplot(data = data2, aes_string(x = pv, y = rv), alpha = 0.7) +
      geom_point(aes(color = region), size = 0.5) +
      geom_smooth(method = 'lm', formula = y ~ x, color = 'light green', se = FALSE) +
      geom_smooth(method = 'loess')
    gs[[i]] <- g
    i <- i + 1
  }
}

pm <- ggmatrix(
  gs, nrow = 2, ncol = 3,
  xAxisLabels = c('age', 'height', 'weight'),
  yAxisLabels = c('FVC', 'FEV1'),
  title = "Pairwise Comparison of Predictors and Responses\n(with the correction for age)"
  )

pm
```

That's a bit better now: *age* does not pull down so much the regression line (against *FVC*, *FEV1*) in its beginning. The corrected dataset looks more suitable for the linear regression now but it has got worse for *height* and *weight* esp. for the latter. There is one case with an unusally low value of *weight* but unusually high values for *FVC*, *FEV1*, the case might be a trained person or a mistake of measurement. At this point, it would be worth to discuss validaty of this case with the owner of dataset.

```{r}
data2 %>% filter(weight == 54)
```

For the time being, let's remove the case and check how the relationship looks without it.

```{r}
data2 <- data2 %>% filter(weight != 54)
```

```{r, fig.width=8, fig.height=6}
gs <-list(NA,NA,NA,NA,NA,NA)

i <- 1
for (rv in c('FVC', 'FEV1')) {
  for (pv in c('age', 'height', 'weight')) {
    g <- ggplot(data = data2, aes_string(x = pv, y = rv), alpha = 0.7) +
      geom_point(aes(color = region), size = 0.5) +
      geom_smooth(method = 'lm', formula = y ~ x, color = 'light green', se = FALSE) +
      geom_smooth(method = 'loess')
    gs[[i]] <- g
    i <- i + 1
  }
}

pm <- ggmatrix(
  gs, nrow = 2, ncol = 3,
  xAxisLabels = c('age', 'height', 'weight'),
  yAxisLabels = c('FVC', 'FEV1'),
  title = "Pairwise Comparison of Predictors and Responses\n(with the correction for age and without one outlier for weight)"
  )

pm
```

That's much better for *weight* now, *age* and *height* still do not look so good but there is no obvious strategy as to normaling them further. We have tried to apply the transformation functions (the power one, log) but that do not help to linearize the relationship.

```{r}
data2['age_sqrt']    <- sqrt(data2['age'])
data2['age_log']     <- log(data2['age'])
data2['age_inv']     <- 1/data2['age']

data2['height_sqrt'] <- sqrt(data2['height'])
data2['height_log']     <- log(data2['height'])

data2['FVC_log']     <- log(data2['FVC'])
```

That's interesting to check the relation between the height and weight just to get an idea.

```{r}
ggplot(data2, aes(x = height, y = weight), alpha = 0.7) +
  geom_smooth(method = 'lm', formula = y ~ x, color = 'light green', se = FALSE) +
  geom_smooth(method = "loess", formula = y ~ x) +
  geom_point(aes(color = region), size = 1)
```

# Linear Regression

## Tests for LM Assumptions

**Linear relationship**. The relationships between the covariates and either of *FVC*, *FEV1* look to be more or less linear though there is some local curvature in the beginning (judging by the 'lowess' line of the regression). As discussed before, it is not clear how to get rid of it therefore we progress with it.

**Normality**. The Shapiro-Wilk normality test for *FVC*, *FEV1* does not reject the null hypothesis. 

```{r}
shapiro.test(data2$FVC)
shapiro.test(data2$FEV1)
```

It's interesting to note that *log(FVC)* does not reject the null hypothesis at a higher level of p-value. Can that suggest a model would benefit of such the transformation for *FVC*? We'll try to check it later.

```{r}
shapiro.test(log(data2$FVC))
```

**Variance**. On the pairwise scatterplots each covariate has values around the regression line more or less evently.

**Multicollinearity** The Pearson's correlation coeffiecients are not large for the pairs of the covariates. We will also try to diagnose violation of this assumption later if that'd be required (say, a model will require most of the variables).

## Choosing a Model for FVC

We build several models for FVC and compare them in terms of the coefficients' significance (alone and together), the value of $R^2$.

```{r}
lm_FVC_1 = lm(data = data2, formula = FVC ~ age + height + weight)
summary(lm_FVC_1)
sprintf('AIC=%.1f', AIC(lm_FVC_1))
```

All predictors are significant except *weight*. That's interesting because one might expect that the volume and strength of lungs should depend on both the height and the weight of a person (as they are strong factors for the volume of body). 

Let's check the model without the weight.

```{r}
lm_FVC_2 = lm(data = data2, formula = FVC ~ age + height)
summary(lm_FVC_2)
sprintf('AIC=%.1f', AIC(lm_FVC_2))
```

It has improved a bit: the coefficients are slightly more statistically significant.

Additionally, checking with ANOVA if *weight* does not improve the model statistically indeed.

```{r}
anova(lm_FVC_1)
```

The interim conclusion is that *FVC* calls only *age* and *height* as the regressors. But let's check if the interaction of *height* and *weight* is significant due to the reason (about the volume) mentioned above.

```{r}
lm_FVC_3 = lm(data = data2, formula = FVC ~ age + height:weight)
summary(lm_FVC_3)
sprintf('AIC=%.1f', AIC(lm_FVC_3))
```

Ahh, the coefficients are more statistically significant now thought R-squared is much worse. This
is not probably a suitable model given difficulties with its interpretation.

Recalling that the log transformation of *FVC* improved the normality test's result, let's check a couple of models for log(FVC) (with and without *weight* as we already known it is not significant but will it become so under the transformation?).

```{r}
lm_FVC_5 = lm(data = data2, formula = log(FVC) ~ age + height + weight)
summary(lm_FVC_5)
sprintf('AIC=%.1f', AIC(lm_FVC_5))
```

```{r}
lm_FVC_6 = lm(data = data2, formula = log(FVC) ~ age + height)
summary(lm_FVC_6)
sprintf('AIC=%.1f', AIC(lm_FVC_6))
```

The model with *log(FVC) ~ age + height* results into more statistically significant coefficients with almost the same R-squared comparing to *FVC ~ age + height*. That's probably the most precise linear regresion model for *FVC*.

Both of them supports that *FVC* increases with *height* and descreases with *age*.

## Choosing a Model for FEV1

Building and comparing several models for *FEV1* using the same approach as for *FVC*.

```{r}
lm_FEV1_1 = lm(data = data2, formula = FEV1 ~ age + height + weight)
summary(lm_FEV1_1)
sprintf('AIC=%.1f', AIC(lm_FEV1_1))
```

The *weight* coefficient is not statistically significant for FEV1 either. Trying a model without it.

```{r}
lm_FEV1_2 = lm(data = data2, formula = FEV1 ~ age + height)
summary(lm_FEV1_2)
sprintf('AIC=%.1f', AIC(lm_FEV1_2))
```

It has improved a bit. Additionally, checking with ANOVA if *weight* is not statistically significant indeed.

```{r}
anova(lm_FEV1_1)
```

## Main Model

Let's choose the model *FVC ~ age + height* in lieu of *FEV1 ~ age + height* because the former has
comparable R-squared and AIC with much more statistically significant intercept. Looking at the
diagnostic plots for it.

```{r, fig.height = 6, fig.width = 8}
autoplot(lm_FVC_2, which = 1:6, ncol = 3, label.size = 3, 
         title = sprintf('Diagnostic Plots for %s', as.character(formula(lm_FVC_2))), alpha = 0.7, size=0.7)
```

The homoscedasticity requirement is fulfilled, that is by judging on the plots "Residuals vs Fitted" and
"Normal Q-Q", the residuals are located on both sides of the middle line and the Q-Q plot looks
quite normal (the residuals are lined along the diagonal). The Scale-Location plot also
supports the assumption of constant variance as the residuals appear on it quite randomly.

There are several leverages but it's hardly possible to do anything with them at the moment without
strong justification for their transformation or even removal, that kind of justification is
expected to be discussed with a subject matter expert (a pulmonologist in this case).

# Box-Cox Transformation

## Finding Optimal Lambda

Running the Box-Cox transformation for a range of lambda's to find an optimal value, for both *FVC* and *FEV1*.

```{r, fig.width=8, fig.height=4}
par(mfrow=c(1,2))

bc_runs_FVC <- boxcox(lm_FVC_2,  lambda = seq(-1, 2, 0.01))
bc_lambda_FVC <- bc_runs_FVC$x[which.max(bc_runs_FVC$y)]
title(main = 'FVC ~ age + height',  sub = sprintf('max(lambda) = %.2f', bc_lambda_FVC),
      cex.main = 1)

bc_runs_FEV1 <- boxcox(lm_FEV1_2, lambda = seq(-1, 2, 0.01))
bc_lambda_FEV1 <- bc_runs_FEV1$x[which.max(bc_runs_FEV1$y)]
title(main = 'FEV1 ~ age + height', sub = sprintf('max(lambda) = %.2f', bc_lambda_FEV1),
      cex.main = 1)

par(mfrow=c(1,1))
```

The Box-Cox transformation suggests using $\lambda=0.11$ for *FVC* and $\lambda=0.74$ for *FEV1*, these are the values when the MLE takes its maximum correspondingly. 

## Models

Let's run the models for both *FVC* and *FEV1* with the suggested transformations.

```{r}
lm_FVC_bc = lm(data = data2, formula = FVC^0.11 ~ age + height)
summary(lm_FVC_bc)
```

```{r}
lm_FEV1_bc = lm(data = data2, formula = FEV1^0.74 ~ age + height)
summary(lm_FEV1_bc)
```

The model for *FEV1* has got the statistically insignificant intercept, we stop considering it. 

The model for *FVC* is slightly improved comparing to the one without the transformation (*FVC ~ age + height*) and is almost the same as *log(FVC) ~ age + height* in terms of t-, F-statistics, R-squared, which is expected given the value of $\lambda$: it is closed to zero and the Box-Cox transformation approximates the logarythmic transformation in this case.

The intercept coefficient has improved most of all comparing to *FVC ~ age + height*.

Let's check the diagnostic plots, they are expected to be simular to those for *FVC ~ age + height*.

```{r, fig.height=6, fig.width=8}
#print(sprintf('Diagnostic Plots for %s', as.character(formula(lm_FVC_bc))))
autoplot(lm_FVC_bc, which = 1:6, ncol = 3, label.size = 3, alpha = 0.7, size=0.7)
```

Yes, they are simular to the model's without the Box-Cox transformation.

## Lambda Significance Test

Conducting tests of significance for the optimal lambda's value ($\lambda = 0.11$) against the
values of 0, 0.5, 1, 2 for *FVC^0.11 ~ age + height*.

```{r}
lm_FVC <- lm(data = data2, formula = FVC ~ age + height, model = FALSE)

# LME for the value of lambda when LME takes the maximum
bc_lme_FVC <- boxcox(lm_FVC, lambda = bc_lambda_FVC,  plotit = FALSE)$y

bc_runs = list(lambda = c(0, 0.5, 1, 2), y = rep(NA, 4))

i <- 1
for (lambda in bc_runs[['lambda']]) {
  bc <- boxcox(lm_FVC, lambda = lambda, plotit = FALSE)
  i <- i + 1
  lrt <- -2*(bc$y - bc_lme_FVC)
  print(sprintf('LRT Statistic for H0: boxcox(lambda=%.2f) == boxcox(lambda=%.2f): %.2f; p-value: %1.5f', 
                lambda, bc_lambda_FVC, lrt, 1 - pchisq(lrt, 1)))
  # Calculate the values based on the slide 41
  bc_runs[['y']][[i]] <- bc$y
}
```

The tests do not reject $H_0$ of no difference between $\lambda$ as 0, 0.5 or 1 and 0.11. It suggests the Box-Cox transformation is not so much necessary, it echoes with the statistics of the two linear regression models.

# Generalized Linear Model

## Choosing a Model for FVC

We run several models for *FVC* and compare them to choose the best.

```{r}
glm_FVC_gs_log_s <- glm(data = data2, FVC ~ age + height + weight + region, family = gaussian(link=log))
summary(glm_FVC_gs_log_s)
```

```{r}
glm_FVC_gs_id_s <- glm(data = data2, FVC ~ age + height + weight + region, family = gaussian(link=identity))
summary(glm_FVC_gs_id_s)
```

```{r}
glm_FVC_G_log_s <- glm(data = data2, FVC ~ age + height + weight + region, family = Gamma(link=log))
summary(glm_FVC_G_log_s)
```

For all three models above, neither *weight* nor *region* are significant. Let's re-check it with ANOVA.

```{r}
anova(glm_FVC_gs_log_s, test="F")
```

```{r}
anova(glm_FVC_G_log_s, test="F")
```

ANOVA supports the view that *weight* and *region* are not significant, their inclusion does not bring the deviance down significantly therefore we exclude both and re-run the three model above but without these two variables.

```{r}
glm_FVC_gs_log <- glm(data = data2, FVC ~ age + height, family = gaussian(link=log))
summary(glm_FVC_gs_log)
```

```{r}
glm_FVC_gs_id <- glm(data = data2, FVC ~ age + height, family = gaussian(link=identity))
summary(glm_FVC_gs_id)
```

```{r}
glm_FVC_G_log <- glm(data = data2, FVC ~ age + height, family = Gamma(link=log))
summary(glm_FVC_G_log)
```

Of all three models above the one with Gamma responses with the log link is the best: the coefficients are more significant, R-squared has got a larger value. But it is just *slightly* better than the one with Gaussian responses with the log link. The model with the identity link function is slightly worse than either of those with the non-identity link functions.

We believe the log link function helps the most in this case, using the Gamma responses (instead of Gaussian) helps less but the differences are not as large as that we should really reject the simplicity of the linear regression model (with the identity link) at the cost of increased complexity of interpretation of the linear regression models (with the log link).

The deviance has slightly increased for all three  models (build on *age*, *height*) only comparing with their corresponding saturated models. We can test for one of those pairs if that's decrease is significant e.g.

```{r}
dev1 <- deviance(glm_FVC_gs_log_s)
dev2 <- deviance(glm_FVC_gs_log)

hatphi <- sum(residuals(glm_FVC_gs_log_s, type="pearson")^2)/glm_FVC_gs_log_s$df.r
F <- ((dev2-dev1)/1)/hatphi
p <- 1-pf(F, 1, glm_FVC_gs_log_s$df.r)

sprintf('deviance 1: %f, deviance 2: %f, F: %f, p-value: %f', dev1, dev2, F, p)
```

The test's result indicates the deviances' difference is not significant.

The best model for *FVC* is the one with the Gamma responses, checking the diagnostic plots for it.

```{r, fig.height=6, fig.width=8}
autoplot(glm_FVC_G_log, which = 1:6, ncol = 3, label.size = 3, alpha = 0.7, size=0.7)
```

The plots looks simular to those for *FVC ~ age + height*, therefore our conclusions about the homoscedacity (the constant variance) stay pretty much the same as before.

## Choosing a Model for FEV1

Let's check if *region* can turn out to be statistically significant for *FEV1* in case of GLM.

```{r}
glm_FEV1_gs_log = glm(data = data2, FEV1 ~ age + height + region, family = gaussian(link=log))
summary(glm_FEV1_gs_log)
```

```{r}
glm_FEV1_G_log = glm(data = data2, FEV1 ~ age + height + region, family = Gamma(link=log))
summary(glm_FEV1_G_log)
```

No, *region* is not statistically significant for FEV1.

However, we have found that *region* is statistically significant for the original data (with the
heavily skewed data for *age*) at the 0.05 significance level (check the model below) but that can explained by the skewness
of *age* for one of regions, which seems to cause the significance. Generalizing it over all the age
ranges deems to be a mistake therefore we reject to accept this statistical significance from the
population's point of view.

```{r}
glm_FEV1_gs_log = glm(data = data, FEV1 ~ age + height + region, family = gaussian(link=log))
summary(glm_FEV1_gs_log)
```

## Models for Region

It would be interesting to try the logistic regression for *region*, can it still be explained by
some of the other variables?

```{r glm}
data['regionM']  <- data['region'] == 'M'
data2['regionM'] <- data2['region'] == 'M'
glm_FVC_bin_logit <- glm(data = data2, regionM ~ FVC + age + height + weight, family = binomial(link=logit))
summary(glm_FVC_bin_logit)
```

The coefficients are not significant, it looks like the data cannot express any difference between
the regions. Again, if we run the model for the original data, only *weight* is statistically
significant but that is misleading for the reasons discussed before (the skewness of cases towards
the lower range of *age* for one of regions).

# Prediction

We choose three GLM models with the formula *FVC ~ age + height* for the prediction, the graph also
shows the observed values (as the black dots).
 
```{r, fig.width=8, fig.height=10}
data_pred <- tibble(age = seq(16, 60))
data_pred['height'] <- 180
data_pred['weight'] <- 80
data_pred['region'] <- 'M'

models <- list(
  model = list(glm_FVC_gs_log, glm_FVC_gs_id, glm_FVC_G_log),
  name =  c('glm_FVC_gs_log', 'glm_FVC_gs_id', 'glm_FVC_G_log')
)

# getting the predictions
for (i in c(1, 2, 3)) {
  
  m <- models$model[[i]]
  pred <- predict(m, newdata = data_pred, type="response", se.fit = T)
  name <- models$name[i]
  data_pred[paste(name, '_fit', sep='')] <- pred$fit
  data_pred[paste(name, '_ci_upper', sep='')] <- pred$fit + qnorm(0.975)*pred$se.fit
  data_pred[paste(name, '_ci_lower', sep='')] <- pred$fit - qnorm(0.975)*pred$se.fit
  
}


# plotting the prediction
colours <- c('Gaussian(log)'='red', 
             'Gaussian(identity)'='green',
             'Gamma(log)'='blue')

g <- ggplot(data = data_pred, aes(x = age)) +
  geom_point(data = data2, aes(x = age, y = FVC), alpha = 0.5) # colour = names(colours)[1]

n <- dim(data_pred)[1]

for (i in c(1, 2, 3)) {
  
  name   <- models$name[i]
  colour <- names(colours)[i]
  g <- g + 
    geom_line(aes_string(y = paste(name, '_fit', sep=''),      color = shQuote(colour)), alpha = 0.5) +
    geom_line(aes_string(y = paste(name, '_ci_upper', sep=''), color = shQuote(colour)), alpha = 0.5) +
    geom_line(aes_string(y = paste(name, '_ci_lower', sep=''), color = shQuote(colour)), alpha = 0.5)

}

g <- g + labs(title = 'Prediction for FVC on age', y = 'FVC (predicted)') +
  scale_color_manual(name = 'Legend', values = colours) +
  theme(legend.position = c(0.8, 0.8))

g

```

There is only a slight visual difference between all three models for *FVC*, the GLM models with the log links are closer to each other, the model with the identity link function tends to predict higher values of *FVC* comparing to the formers.

# Conclusions

1. The population have probably been sampled wrongly because *age* is heavily skewed for Aichfeld, there are less observations for Aichfeld especially after after the correction of the *age* skewness.
2. There is strong suspicion that a single linear model won't fit all age ranges for either *FVC* or *FEV1* as for younger age ranges (which means smaller bodies with smaller lungs) *FVC*, *FEV1* will naturally be lower.
3. Of all covariates, only *age* and *height* have turned out to be statistically significant in all models incl. the GLM ones.
4. The Box-Cox transformation and usage of GLM improves the goodness-of-fit only slightly. It might mean the data is already good enough for the linear regression without the transformations. However, we still learn from the project that GLM allows building more precise models comparing to the linear regression.

# Ideas

1. Though the most accurate models have not shown any statistically significant difference between the regions, one of models has shown it. It might be worth to check how the data was collected, whether it can be corrected additionally to re-do the modelling. Also, it can motivate for further investigations into the research question to gather better data for getting more precise answers.
2. Given the non-linearity of *FVC* ~ *age* or *FEV1* ~ *age*, it's worth considering building different models for two different age ranges: one model for up to the 25 years inclusive (there is the *positive* association within that age range), and another model for after the 25 years (there is the *negative* association withing that age range).
3. Regress on the volume of body rather than on just the height and/or the weight because the common sense suggests that the lung function depends on the volume of body to a greater extent. A quick search with Google reveals there are works on calculating the body volume using the above-mentioned two dimensions e.g. Sendroy Jr, Julius, and Harold A. Collison. "Determination of human body volume from height and weight." Journal of Applied Physiology 21, no. 1 (1966): 167-172.
4. The data seems to have been collected over several years so it might be worth to take the time difference into the account. Listing the number of cases for different years by the regions.

```{r}
data[, c('year', 'region')] %>% 
  count(year, region) %>% 
  spread(key = region, value = n, fill = '-') %>% 
  arrange(year) %>% 
  print(n = 100)
```

# Appendix A. Technical Details

This version of the report was built with the following runtime environment:

```{r}
devtools::session_info()
```


